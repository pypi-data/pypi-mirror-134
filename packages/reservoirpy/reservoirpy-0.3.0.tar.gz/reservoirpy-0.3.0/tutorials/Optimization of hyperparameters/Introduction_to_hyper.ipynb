{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to hyperparameters optimization with *hyperopt* and ReservoirPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from reservoirpy import ESN, mat_gen\n",
    "from reservoirpy.hyper import research, plot_hyperopt_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Double-scroll attractor\n",
    "\n",
    "The double-scroll attractor or Chua's attractor is defined be the following differential equations:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{dx}{dt} &= a(y(t) - x(t)) \\\\\n",
    "\\frac{dy}{dt} &= (c - a)x(t) - x(t)z(t) + cy(t) \\\\\n",
    "\\frac{dz}{dt} &= x(t)y(t) - bz(t)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "These equations describes a kind of electrical devices, which are known to have a chaotic behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reservoirpy.datasets import multiscroll\n",
    "\n",
    "timesteps = 10000\n",
    "X = multiscroll(timesteps)\n",
    "\n",
    "# rescale between -1 and 1\n",
    "X = 2 * (X - X.min()) / (X.max() - X.min()) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multiscroll(X, sample=2000):\n",
    "    \n",
    "    t = np.arange(X.shape[0])\n",
    "    fig = plt.figure(figsize=(13, 5))\n",
    "    plt.margins(0.05)\n",
    "\n",
    "    plt.subplot(121)\n",
    "    plt.plot(t[:sample], X[:sample, 0], color=\"lightgray\", zorder=0)\n",
    "    plt.scatter(t[:sample], X[:sample, 0], cmap=\"viridis\", c=t[:sample], s=0.5)\n",
    "    plt.xlabel(\"$t$\")\n",
    "    plt.ylabel(\"$P(t)$\")\n",
    "    plt.title(\"Multiscroll attractor\")\n",
    "\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.ax.set_xlabel('$t$')\n",
    "\n",
    "    ax = plt.subplot(122, projection=\"3d\")\n",
    "    plt.plot(X[:sample, 0], X[:sample, 1], X[:sample, 2], color=\"lightgray\", zorder=0, lw=0.5)\n",
    "    plt.scatter(X[:sample, 0], X[:sample, 1], zs=X[:sample, 2], cmap=\"viridis\", c=t[:sample], s=0.8)\n",
    "    plt.title(\"Phase diagram\")\n",
    "    plt.xlabel(\"$x$\")\n",
    "    plt.ylabel(\"$y$\")\n",
    "    ax.set_zlabel(\"$z$\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multiscroll(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: define the objective\n",
    "\n",
    "The first steps consists in defining the objective function you want to optimize. This is the most important step: you must define an experimentation which is reproducible and which will produce results that can be measured to approximate the function you want to optimize.\n",
    "\n",
    "Most optimization algorithms relies on the hypothesis of *convexity* (like most estimators in machine learning). In our case, that means that *hyperopt* expects that the objective function have at least some local minima that can be reached by shifting the parameters.\n",
    "\n",
    "We therefore chose $RMSE$ (Root Mean Squared Error) as a *loss function*, the function that will be used within the objective function to evaluate the quality of the parameters we chose. We can make the assumption that this function, combined with the model function of the ESN, has a least some local minima without taking to much risks. Of course, we do not know the shape of this function, and we can't \"plot it\" to see where the minimum is. This is why we will rely on tools like *hyperopt* to approximate this function in many points, and empirically find a minimum.\n",
    "\n",
    "In addition to the loss function, we also compute an other metric, the $R^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_score(y_true, y_pred):\n",
    "    return 1 - (np.sum((y_true - y_pred)**2) / np.sum((y_true - y_true.mean())**2))\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt((np.sum(y_true - y_pred)**2) / len(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objective functions accepted by ReservoirPy must respect some conventions:\n",
    "#  - dataset and config arguments are mandatory, like the empty '*' expression.\n",
    "#  - all parameters that will be used during the search must be placed after the *.\n",
    "#  - the function must return a dict with at least a 'loss' key containing the result\n",
    "# of the loss function. You can add any additional metrics or information with other \n",
    "# keys in the dict.\n",
    "def objective(dataset, config, *, iss, N, sr, leak, ridge, seed):\n",
    "        \n",
    "    # unpack train and test data, with target values.\n",
    "    # data will be splitted in two folds.\n",
    "    num_folds = 2\n",
    "    \n",
    "    train_data, test_data = dataset\n",
    "    (X_train, y_train), (X_train2, y_train2) = train_data # fold 1\n",
    "    (X_test, y_test), (X_test2, y_test2) = test_data # fold 2\n",
    "    \n",
    "    X_trains = X_train, X_train2\n",
    "    X_tests = X_test, X_test2\n",
    "    y_trains = y_train, y_train2\n",
    "    y_tests = y_test, y_test2\n",
    "\n",
    "    nb_features = X_train.shape[1]\n",
    "\n",
    "    instances = config[\"instances_per_trial\"]\n",
    "\n",
    "    variable_seed = seed # the seed will be changed across the instances, always in the \n",
    "                         # way across the trials, to be sure there is no bias in the results\n",
    "                         # due to initialization.\n",
    "    \n",
    "    losses = []; r2 = [];\n",
    "    for n in range(instances):\n",
    "        for i in range(num_folds):\n",
    "            # builds an ESN given the input parameters\n",
    "            W = mat_gen.fast_spectral_initialization(N=N, sr=sr,\n",
    "                                                     seed=variable_seed)\n",
    "\n",
    "            Win = mat_gen.generate_input_weights(N=N, dim_input=nb_features,\n",
    "                                                 input_bias=True, input_scaling=iss,\n",
    "                                                 seed=variable_seed+1)\n",
    "\n",
    "\n",
    "            reservoir = ESN(lr=leak, W=W, Win=Win, input_bias=True, ridge=ridge)\n",
    "\n",
    "\n",
    "            # train and test the model\n",
    "            reservoir.train([X_trains[i]], [y_trains[i]], verbose=False)\n",
    "        \n",
    "            seed_timesteps = 100\n",
    "            warming_inputs = X_tests[i][:seed_timesteps]\n",
    "            generations = X_tests[i].shape[0] - seed_timesteps\n",
    "            outputs, _, _, _ = reservoir.generate(generations, warming_inputs=warming_inputs, verbose=False)\n",
    "\n",
    "            losses.append(rmse(outputs[0], y_tests[i][:seed_timesteps]))\n",
    "            r2.append(r2_score(outputs[0], y_tests[i][:seed_timesteps]))\n",
    "        \n",
    "        variable_seed += 1 # shift the seed between instances\n",
    "\n",
    "    # returns a dictionnary of metrics. The 'loss' key is mandatory when\n",
    "    # using hyperopt.\n",
    "    return {'loss': np.mean(losses),\n",
    "            'r2': np.mean(r2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: define the research space\n",
    "\n",
    "We can now define how we want hyperopt to find a minimum of the loss function, and what parameters we want to optimize.\n",
    "\n",
    "Hyperopt configuration files can be written as shown in the next cell. The \"exp\", \"hp_max_evals\", \"hp_method\" and \"hp_space\" are mandatory. All the other keys are optionnal, like \"seed\", or are user-defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentation_no = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperopt_config = {\n",
    "    \"exp\": f\"{experimentation_no}-hyperopt-multiscroll\", # the experimentation name\n",
    "    \"hp_max_evals\": 200,             # the number of differents sets of parameters hyperopt has to try\n",
    "    \"hp_method\": \"random\",           # the method used by hyperopt to chose those sets (see below)\n",
    "    \"seed\": 42,                      # the random state seed, to ensure reproducibility\n",
    "    \"instances_per_trial\": 3,        # how many random ESN will be tried with each sets of parameters\n",
    "    \"hp_space\": {                    # what are the ranges of parameters explored\n",
    "        \"N\": [\"choice\", 500],             # the number of neurons is fixed to 300\n",
    "        \"sr\": [\"loguniform\", 1e-2, 10],   # the spectral radius is log-uniformly distributed between 1e-6 and 10\n",
    "        \"leak\": [\"loguniform\", 1e-3, 1],  # idem with the leaking rate, from 1e-3 to 1\n",
    "        \"iss\": [\"choice\", 0.9],           # the input scaling is fixed\n",
    "        \"ridge\": [\"choice\", 1e-7],        # and so is the regularization parameter.\n",
    "        \"seed\": [\"choice\", 1234]          # an other random seed for the ESN initialization\n",
    "    }\n",
    "}\n",
    "\n",
    "# we precautionously save the configuration in a JSON file\n",
    "# each file will begin with a number corresponding to the current experimentation run number.\n",
    "with open(f\"{hyperopt_config['exp']}.config.json\", \"w+\") as f:\n",
    "    json.dump(hyperopt_config, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend using random search algorithm. Indeed, by randomly chosing the parameters within a specifyed range, we maximize our chances to reach a local minimum. Using a grid search would add a bias during the optimization, which is the fixed gap between two consecutive values of parameters. This gap could be too big and prevent hyperopt from finding a relevant minimum, by always making the loss \"jump accross\" that minimum. With a random distribution of parameters and enough trials, there is a chance that the loss make a sufficiently little jump to reach the minimum at least once.\n",
    "\n",
    "We also encourage you to fix the maximum of parameters possible. You should never try to optimize all parameters at once during one huge experimentation. You will end up dealing with all the possible interactions between the parameters, making the task of chosing a relevant set of parameters very difficult. \n",
    "\n",
    "You should rather run several little experimentations where you shift only two or three parameters. By always chosing the best parameters at each iteration, you will end with an optimized set of parameters, which might not be the best one ever, but a robust and well tested one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: prepare the data\n",
    "\n",
    "We will split the data in 2 different folds, to make the search more robust: one fold with the testing set starting from the begining of the timeseries, one fold with the testing set starting near the end of the timeseries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_len = 200\n",
    "\n",
    "X_train = X[:-test_len-1]\n",
    "y_train = X[1: -test_len]\n",
    "\n",
    "X_test = X[-test_len-1:-1]\n",
    "y_test = X[-test_len:]\n",
    "\n",
    "X_train2 = X[test_len-1:-1]\n",
    "y_train2 = X[test_len:]\n",
    "\n",
    "X_test2 = X[:test_len-1]\n",
    "y_test2 = X[1:test_len]\n",
    "\n",
    "dataset = (((X_train, y_train), (X_train2, y_train2)), \n",
    "           ((X_test, y_test), (X_test2, y_test2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: launch *hyperopt*\n",
    "\n",
    "This might take some time... You can skip this step and directly see the results in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentation_no += 1\n",
    "\n",
    "# run the random search\n",
    "best = research(objective, dataset, f\"{hyperopt_config['exp']}.config.json\", \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: choose parameters\n",
    "\n",
    "We can then use ReservoirPy plotting method to quickly display the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plot_hyperopt_report(hyperopt_config[\"exp\"], (\"leak\", \"sr\"), metric=\"r2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this plot, each orange dot represents an approximation of the loss, as a function of one parameter at a time. The red dot is the lowest loss, and the green dots are the bests values for the chosen metric, here the $R^2$ score. The other scatter plots show the interaction of the parameters with themselves and the loss.\n",
    "The violin plots below help giving an overview of the parameter distribution of the bests results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next task: make a forecast for COVID data\n",
    "\n",
    "Data can be found at www.data.gouv.fr.\n",
    "\n",
    "We want to make a forecast of 4 indicators giving insight on the spread of the COVID epidemic in France."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = requests.get(\"https://www.data.gouv.fr/fr/datasets/r/d2671c6c-c0eb-4e12-b69a-8e8f87fc224c\").json()\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df = df.fillna(0)\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"]) \n",
    "df = df.sort_values(by=\"date\").reset_index(drop=True)\n",
    "\n",
    "feat_names = ['reanimation', 'hospitalises',  # hostpitalizations features names\n",
    "              'nouvellesHospitalisations', 'nouvellesReanimations']  # intensive care features names\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataframe(features, df):\n",
    "    matplotlib.rc_file_defaults()\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i, feat in enumerate(feat_names):\n",
    "        plt.plot(df[feat], label=feat)\n",
    "    ticks = [0, 50, 100, 150, 200, 250]\n",
    "    plt.xticks(ticks, [df[\"date\"].loc[i].date() for i in ticks], rotation=45)\n",
    "    plt.ylabel(\"Features\")\n",
    "    plt.legend(); plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dataframe(feat_names, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_roll = np.array([df[f].rolling(5, win_type=\"hamming\").mean().fillna(0).values for f in feat_names]).T\n",
    "X = (X_roll - X_roll.min(axis=0)) / (X_roll.max(axis=0) - X_roll.min(axis=0))\n",
    "\n",
    "matplotlib.rc_file_defaults()\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i, feat in enumerate(feat_names):\n",
    "    plt.plot(X[:, i], label=feat)\n",
    "ticks = [0, 50, 100, 150, 200, 250]\n",
    "plt.xticks(ticks, [df[\"date\"].loc[i].date() for i in ticks], rotation=45)\n",
    "plt.ylabel(\"Normalized features\")\n",
    "plt.legend(); plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_len = 200\n",
    "\n",
    "X_train = X[:-test_len-1]\n",
    "y_train = X[1: -test_len]\n",
    "\n",
    "X_test = X[-test_len-1:-1]\n",
    "y_test = X[-test_len:]\n",
    "\n",
    "\n",
    "dataset = (X_train, y_train), (X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the search\n",
    "\n",
    "We define the task the same way as before. We will reuse the hyperopt configuration and the objective function used for double-scroll attractor generation task. Feel free to tune the hyperparameter search your own way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentation_no = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_config = {\n",
    "    \"exp\": f\"{experimentation_no}-hyperopt-covid\", # the experimentation name\n",
    "    \"hp_max_evals\": 500,             # the number of differents sets of parameters hyperopt has to try\n",
    "    \"hp_method\": \"random\",           # the method used by hyperopt to chose those sets (see below)\n",
    "    \"seed\": 42,                      # the random state seed, to ensure reproducibility\n",
    "    \"instances_per_trial\": 3,        # how many random ESN will be tried with each sets of parameters\n",
    "    \"hp_space\": {                    # what are the ranges of parameters explored\n",
    "        \"N\": [\"choice\", 500],             # the number of neurons is fixed to 300\n",
    "        \"sr\": [\"loguniform\", 1e-2, 10],   # the spectral radius is log-uniformly distributed between 1e-6 and 10\n",
    "        \"leak\": [\"loguniform\", 1e-3, 1],  # idem with the leaking rate, from 1e-3 to 1\n",
    "        \"iss\": [\"choice\", 0.9],           # the input scaling is fixed\n",
    "        \"ridge\": [\"choice\", 1e-7],        # and so is the regularization parameter.\n",
    "        \"seed\": [\"choice\", 1234]          # an other random seed for the ESN initialization\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "with open(f\"{covid_config['exp']}.config.json\", \"w+\") as f:\n",
    "    json.dump(covid_config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covid_objective(dataset, config, *, iss, N, sr, leak, ridge, seed):\n",
    "        \n",
    "    # unpack train and test data, with target values.\n",
    "    \n",
    "    train_data, test_data = dataset\n",
    "    \n",
    "    X_train, y_train = train_data\n",
    "    X_test, y_test = test_data\n",
    "\n",
    "\n",
    "    nb_features = X_train.shape[1]\n",
    "\n",
    "    instances = config[\"instances_per_trial\"]\n",
    "\n",
    "    variable_seed = seed # the seed will be changed across the instances, always in the \n",
    "                         # way across the trials, to be sure there is no bias in the results\n",
    "                         # due to initialization.\n",
    "    \n",
    "    losses = []; r2 = [];\n",
    "    for n in range(instances):\n",
    "        # builds an ESN given the input parameters\n",
    "        W = mat_gen.fast_spectral_initialization(N=N, sr=sr,\n",
    "                                                 seed=variable_seed)\n",
    "\n",
    "        Win = mat_gen.generate_input_weights(N=N, dim_input=nb_features,\n",
    "                                             input_bias=True, input_scaling=iss,\n",
    "                                             seed=variable_seed+1)\n",
    "\n",
    "\n",
    "        reservoir = ESN(lr=leak, W=W, Win=Win, input_bias=True, ridge=ridge)\n",
    "\n",
    "\n",
    "        # train and test the model\n",
    "        reservoir.train([X_train], [y_train], verbose=False)\n",
    "\n",
    "        warming_inputs = X_train\n",
    "        generations = len(X_test)\n",
    "        outputs, _, _, _ = reservoir.generate(generations, warming_inputs=warming_inputs, verbose=False)\n",
    "\n",
    "        losses.append(rmse(outputs[0], y_test))\n",
    "        r2.append(r2_score(outputs[0], y_test))\n",
    "        \n",
    "        variable_seed += 1 # shift the seed between instances\n",
    "\n",
    "    # returns a dictionnary of metrics. The 'loss' key is mandatory when\n",
    "    # using hyperopt.\n",
    "    return {'loss': np.mean(losses),\n",
    "            'r2': np.mean(r2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_length = len(df) - 31\n",
    "\n",
    "X_train = X[:train_length]\n",
    "y_train = X[1: train_length+1]\n",
    "\n",
    "X_test = X[train_length:-1]\n",
    "y_test = X[train_length+1:]\n",
    "\n",
    "\n",
    "covid_dataset = (X_train, y_train), (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentation_no += 1\n",
    "\n",
    "# run the random search\n",
    "best = research(covid_objective, covid_dataset, f\"{covid_config['exp']}.config.json\", \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_hyperopt_report(covid_config[\"exp\"], (\"leak\", \"sr\"), metric=\"r2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = 1000\n",
    "leak_rate = 0.23\n",
    "spectral_radius = 3.95\n",
    "iss = 0.9\n",
    "density = 0.1\n",
    "input_connectivity = 0.1\n",
    "regularization = 1e-7\n",
    "seed = 1234\n",
    "\n",
    "W = mat_gen.fast_spectral_initialization(units, sr=spectral_radius,\n",
    "                              proba=density, seed=seed)\n",
    "\n",
    "Win = mat_gen.generate_input_weights(units, 4, input_scaling=iss, \n",
    "                                     proba=input_connectivity, input_bias=True,\n",
    "                                     seed=seed+1)\n",
    "\n",
    "reservoir = ESN(leak_rate, W, Win, ridge=regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = reservoir.train([X_train], [y_train], verbose=True, wash_nr_time_step=0)\n",
    "\n",
    "y_pred, states, _, _ = reservoir.generate(X_test.shape[0], warming_inputs=X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results():\n",
    "    matplotlib.rc_file_defaults()\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    color = matplotlib.rcParams['axes.prop_cycle'][:len(feat_names)]\n",
    "    \n",
    "    ax1 = fig.add_subplot(111)\n",
    "    ax1.set_prop_cycle(color)\n",
    "    ax1.set_title(f\"{len(y_test)} days forecast of Covid hospitalization indicators\")\n",
    "    ax1.plot(y_pred)\n",
    "    ax1.plot(y_test, linestyle=\"--\")\n",
    "    ax1.plot([],[], color=\"black\", label=\"Generated\")\n",
    "    ax1.plot([],[], linestyle=\"--\", color=\"black\", label=\"Real\")\n",
    "    ax1.plot([],[],' ', label=f\"$R^2={round(r2_score(y_test, y_pred), 4)}$\")\n",
    "    ax1.plot([],[],' ', label=f\"$RMSE={round(rmse(y_test, y_pred), 4)}$\")\n",
    "    ax1.set_ylabel(\"Normalized indicator\")\n",
    "    ticks = [i for i in range(0, len(y_test), 5)]\n",
    "    ax1.set_xticks(ticks)\n",
    "    ax1.set_xticklabels([df[\"date\"].loc[len(y_train) + i].date() for i in ticks], rotation=45)\n",
    "    ax1.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}