---

title: Factorization


keywords: fastai
sidebar: home_sidebar

summary: "Matrix Factorization algorithmic modules for recommender systems."
description: "Matrix Factorization algorithmic modules for recommender systems."
nb_path: "nbs/models/factorization.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/models/factorization.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="MF" class="doc_header"><code>class</code> <code>MF</code><a href="https://github.com/RecoHut-Projects/recohut/tree/master/recohut/models/factorization.py#L11" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>MF</code>(<strong><code>num_users</code></strong>, <strong><code>num_items</code></strong>, <strong><code>emb_size</code></strong>=<em><code>100</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="BiasedMF" class="doc_header"><code>class</code> <code>BiasedMF</code><a href="https://github.com/RecoHut-Projects/recohut/tree/master/recohut/models/factorization.py#L25" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>BiasedMF</code>(<strong><code>num_users</code></strong>, <strong><code>num_items</code></strong>, <strong><code>emb_size</code></strong>=<em><code>100</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="NeuMF" class="doc_header"><code>class</code> <code>NeuMF</code><a href="https://github.com/RecoHut-Projects/recohut/tree/master/recohut/models/factorization.py#L45" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>NeuMF</code>(<strong><code>args</code></strong>, <strong><code>num_users</code></strong>, <strong><code>num_items</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="GMF" class="doc_header"><code>class</code> <code>GMF</code><a href="https://github.com/RecoHut-Projects/recohut/tree/master/recohut/models/factorization.py#L105" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>GMF</code>(<strong><code>args</code></strong>, <strong><code>num_users</code></strong>, <strong><code>num_items</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># import torch.nn.functional as F</span>

<span class="c1"># from recohut.datasets.synthetic import Synthetic</span>
<span class="c1"># from recohut.transforms.split import chrono_split</span>
<span class="c1"># from recohut.transforms.encode import label_encode as le</span>
<span class="c1"># from recohut.models.dnn import CollabFNet</span>

<span class="c1"># # generate synthetic implicit data</span>
<span class="c1"># synt = Synthetic()</span>
<span class="c1"># df = synt.implicit()</span>

<span class="c1"># # drop duplicates</span>
<span class="c1"># df = df.drop_duplicates()</span>

<span class="c1"># # chronological split</span>
<span class="c1"># df_train, df_valid = chrono_split(df)</span>
<span class="c1"># print(f&quot;Train set:\n\n{df_train}\n{&#39;=&#39;*100}\n&quot;)</span>
<span class="c1"># print(f&quot;Validation set:\n\n{df_valid}\n{&#39;=&#39;*100}\n&quot;)</span>

<span class="c1"># # label encoding</span>
<span class="c1"># df_train, uid_maps = le(df_train, col=&#39;USERID&#39;)</span>
<span class="c1"># df_train, iid_maps = le(df_train, col=&#39;ITEMID&#39;)</span>
<span class="c1"># df_valid = le(df_valid, col=&#39;USERID&#39;, maps=uid_maps)</span>
<span class="c1"># df_valid = le(df_valid, col=&#39;ITEMID&#39;, maps=iid_maps)</span>

<span class="c1"># # event implicit to rating conversion</span>
<span class="c1"># event_weights = {&#39;click&#39;:1, &#39;add&#39;:2, &#39;purchase&#39;:4}</span>
<span class="c1"># event_maps = dict({&#39;EVENT_TO_IDX&#39;:event_weights})</span>
<span class="c1"># df_train = le(df_train, col=&#39;EVENT&#39;, maps=event_maps)</span>
<span class="c1"># df_valid = le(df_valid, col=&#39;EVENT&#39;, maps=event_maps)</span>
<span class="c1"># print(f&quot;Processed Train set:\n\n{df_train}\n{&#39;=&#39;*100}\n&quot;)</span>
<span class="c1"># print(f&quot;Processed Validation set:\n\n{df_valid}\n{&#39;=&#39;*100}\n&quot;)</span>

<span class="c1"># # get number of unique users and items</span>
<span class="c1"># num_users = len(df_train.USERID.unique())</span>
<span class="c1"># num_items = len(df_train.ITEMID.unique())</span>
<span class="c1"># print(f&quot;There are {num_users} users and {num_items} items.\n{&#39;=&#39;*100}\n&quot;)</span>

<span class="c1"># # training and testing related helper functions</span>
<span class="c1"># def train_epocs(model, epochs=10, lr=0.01, wd=0.0, unsqueeze=False):</span>
<span class="c1">#     optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)</span>
<span class="c1">#     model.train()</span>
<span class="c1">#     for i in range(epochs):</span>
<span class="c1">#         users = torch.LongTensor(df_train.USERID.values) # .cuda()</span>
<span class="c1">#         items = torch.LongTensor(df_train.ITEMID.values) #.cuda()</span>
<span class="c1">#         ratings = torch.FloatTensor(df_train.EVENT.values) #.cuda()</span>
<span class="c1">#         if unsqueeze:</span>
<span class="c1">#             ratings = ratings.unsqueeze(1)</span>
<span class="c1">#         y_hat = model(users, items)</span>
<span class="c1">#         loss = F.mse_loss(y_hat, ratings)</span>
<span class="c1">#         optimizer.zero_grad()</span>
<span class="c1">#         loss.backward()</span>
<span class="c1">#         optimizer.step()</span>
<span class="c1">#         print(loss.item()) </span>
<span class="c1">#     test_loss(model, unsqueeze)</span>

<span class="c1"># def test_loss(model, unsqueeze=False):</span>
<span class="c1">#     model.eval()</span>
<span class="c1">#     users = torch.LongTensor(df_valid.USERID.values) #.cuda()</span>
<span class="c1">#     items = torch.LongTensor(df_valid.ITEMID.values) #.cuda()</span>
<span class="c1">#     ratings = torch.FloatTensor(df_valid.EVENT.values) #.cuda()</span>
<span class="c1">#     if unsqueeze:</span>
<span class="c1">#         ratings = ratings.unsqueeze(1)</span>
<span class="c1">#     y_hat = model(users, items)</span>
<span class="c1">#     loss = F.mse_loss(y_hat, ratings)</span>
<span class="c1">#     print(&quot;test loss %.3f &quot; % loss.item())</span>

<span class="c1"># # training MF model</span>
<span class="c1"># model = MF(num_users, num_items, emb_size=100) # .cuda() if you have a GPU</span>
<span class="c1"># print(f&quot;Training MF model:\n&quot;)</span>
<span class="c1"># train_epocs(model, epochs=10, lr=0.1)</span>
<span class="c1"># print(f&quot;\n{&#39;=&#39;*100}\n&quot;)</span>

<span class="c1"># # training MF with bias model</span>
<span class="c1"># model = MF_bias(num_users, num_items, emb_size=100) #.cuda()</span>
<span class="c1"># print(f&quot;Training MF+bias model:\n&quot;)</span>
<span class="c1"># train_epocs(model, epochs=10, lr=0.05, wd=1e-5)</span>
<span class="c1"># print(f&quot;\n{&#39;=&#39;*100}\n&quot;)</span>

<span class="c1"># # training MLP model</span>
<span class="c1"># model = CollabFNet(num_users, num_items, emb_size=100) #.cuda()</span>
<span class="c1"># print(f&quot;Training MLP model:\n&quot;)</span>
<span class="c1"># train_epocs(model, epochs=15, lr=0.05, wd=1e-6, unsqueeze=True)</span>
<span class="c1"># print(f&quot;\n{&#39;=&#39;*100}\n&quot;)</span>


<span class="c1"># &quot;&quot;&quot;</span>
<span class="c1"># Train set:</span>
<span class="c1">#     USERID  ITEMID     EVENT   TIMESTAMP</span>
<span class="c1"># 0        1       1     click  2000-01-01</span>
<span class="c1"># 2        1       2     click  2000-01-02</span>
<span class="c1"># 5        2       1     click  2000-01-01</span>
<span class="c1"># 6        2       2  purchase  2000-01-01</span>
<span class="c1"># 7        2       1       add  2000-01-03</span>
<span class="c1"># 8        2       2  purchase  2000-01-03</span>
<span class="c1"># 10       3       3     click  2000-01-01</span>
<span class="c1"># 11       3       3     click  2000-01-03</span>
<span class="c1"># 12       3       3       add  2000-01-03</span>
<span class="c1"># 13       3       3  purchase  2000-01-03</span>
<span class="c1"># ====================================================================================================</span>
<span class="c1"># Validation set:</span>
<span class="c1">#     USERID  ITEMID     EVENT   TIMESTAMP</span>
<span class="c1"># 4        1       2  purchase  2000-01-02</span>
<span class="c1"># 9        2       3  purchase  2000-01-03</span>
<span class="c1"># 14       3       1     click  2000-01-04</span>
<span class="c1"># ====================================================================================================</span>
<span class="c1"># Processed Train set:</span>
<span class="c1">#     USERID  ITEMID  EVENT   TIMESTAMP</span>
<span class="c1"># 0        0       0      1  2000-01-01</span>
<span class="c1"># 2        0       1      1  2000-01-02</span>
<span class="c1"># 5        1       0      1  2000-01-01</span>
<span class="c1"># 6        1       1      4  2000-01-01</span>
<span class="c1"># 7        1       0      2  2000-01-03</span>
<span class="c1"># 8        1       1      4  2000-01-03</span>
<span class="c1"># 10       2       2      1  2000-01-01</span>
<span class="c1"># 11       2       2      1  2000-01-03</span>
<span class="c1"># 12       2       2      2  2000-01-03</span>
<span class="c1"># 13       2       2      4  2000-01-03</span>
<span class="c1"># ====================================================================================================</span>
<span class="c1"># Processed Validation set:</span>
<span class="c1">#     USERID  ITEMID  EVENT   TIMESTAMP</span>
<span class="c1"># 4        0       1      4  2000-01-02</span>
<span class="c1"># 9        1       2      4  2000-01-03</span>
<span class="c1"># 14       2       0      1  2000-01-04</span>
<span class="c1"># ====================================================================================================</span>
<span class="c1"># There are 3 users and 3 items.</span>
<span class="c1"># ====================================================================================================</span>
<span class="c1"># Training MF model:</span>
<span class="c1"># 5.836816787719727</span>
<span class="c1"># 1.993103265762329</span>
<span class="c1"># 4.549840450286865</span>
<span class="c1"># 1.5779536962509155</span>
<span class="c1"># 1.285771131515503</span>
<span class="c1"># 1.926152229309082</span>
<span class="c1"># 2.242276191711426</span>
<span class="c1"># 2.270019054412842</span>
<span class="c1"># 2.3635096549987793</span>
<span class="c1"># 2.272618055343628</span>
<span class="c1"># test loss 9.208 </span>
<span class="c1"># ====================================================================================================</span>
<span class="c1"># Training MF+bias model:</span>
<span class="c1"># 5.8399200439453125</span>
<span class="c1"># 3.7661311626434326</span>
<span class="c1"># 1.8716331720352173</span>
<span class="c1"># 1.6015545129776</span>
<span class="c1"># 1.5306222438812256</span>
<span class="c1"># 1.2995147705078125</span>
<span class="c1"># 1.1046849489212036</span>
<span class="c1"># 1.1331274509429932</span>
<span class="c1"># 1.2109991312026978</span>
<span class="c1"># 1.2451963424682617</span>
<span class="c1"># test loss 5.625 </span>
<span class="c1"># ====================================================================================================</span>
<span class="c1"># Training MLP model:</span>
<span class="c1"># 4.953568458557129</span>
<span class="c1"># 9.649873733520508</span>
<span class="c1"># 1.1805670261383057</span>
<span class="c1"># 2.54287052154541</span>
<span class="c1"># 2.6113314628601074</span>
<span class="c1"># 2.1839144229888916</span>
<span class="c1"># 1.4144573211669922</span>
<span class="c1"># 0.8893814086914062</span>
<span class="c1"># 0.7603365182876587</span>
<span class="c1"># 1.240354061126709</span>
<span class="c1"># 1.1316341161727905</span>
<span class="c1"># 0.8014519810676575</span>
<span class="c1"># 0.7997692823410034</span>
<span class="c1"># 0.8474739789962769</span>
<span class="c1"># 0.9691768884658813</span>
<span class="c1"># test loss 5.207 </span>
<span class="c1"># ====================================================================================================</span>
<span class="c1"># &quot;&quot;&quot;</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

</div>
 

