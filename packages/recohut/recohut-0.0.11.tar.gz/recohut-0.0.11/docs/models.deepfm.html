---

title: DeepFM


keywords: fastai
sidebar: home_sidebar

summary: "A pytorch implementation of DeepFM."
description: "A pytorch implementation of DeepFM."
nb_path: "nbs/models/models.deepfm.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/models/models.deepfm.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="v1">v1<a class="anchor-link" href="#v1"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote><p><strong>References:-</strong>- H Guo, et al. DeepFM: A Factorization-Machine based Neural Network for CTR Prediction, 2017.</p>
<ul>
<li><a href="https://github.com/rixwew/pytorch-fm/blob/master/torchfm/model/dfm.py">https://github.com/rixwew/pytorch-fm/blob/master/torchfm/model/dfm.py</a></li>
</ul>
</blockquote>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="FactorizationMachine" class="doc_header"><code>class</code> <code>FactorizationMachine</code><a href="https://github.com/RecoHut-Projects/recohut/tree/master/recohut/models/deepfm.py#L11" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>FactorizationMachine</code>(<strong><code>reduce_sum</code></strong>=<em><code>True</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="DeepFM" class="doc_header"><code>class</code> <code>DeepFM</code><a href="https://github.com/RecoHut-Projects/recohut/tree/master/recohut/models/deepfm.py#L28" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>DeepFM</code>(<strong><code>field_dims</code></strong>, <strong><code>embed_dim</code></strong>, <strong><code>mlp_dims</code></strong>, <strong><code>dropout</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>A pytorch implementation of DeepFM.
Reference:
    H Guo, et al. DeepFM: A Factorization-Machine based Neural Network for CTR Prediction, 2017.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="v2">v2<a class="anchor-link" href="#v2"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote><p><strong>References:-</strong>- <a href="https://github.com/huangjunheng/recommendation_model/tree/master/deepFM">https://github.com/huangjunheng/recommendation_model/tree/master/deepFM</a></p>
</blockquote>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="DeepFMv2" class="doc_header"><code>class</code> <code>DeepFMv2</code><a href="https://github.com/RecoHut-Projects/recohut/tree/master/recohut/models/deepfm.py#L78" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>DeepFMv2</code>(<strong><code>feat_sizes</code></strong>, <strong><code>sparse_feature_columns</code></strong>, <strong><code>dense_feature_columns</code></strong>, <strong><code>dnn_hidden_units</code></strong>=<em><code>[400, 400, 400]</code></em>, <strong><code>dnn_dropout</code></strong>=<em><code>0.0</code></em>, <strong><code>ebedding_size</code></strong>=<em><code>4</code></em>, <strong><code>l2_reg_linear</code></strong>=<em><code>1e-05</code></em>, <strong><code>l2_reg_embedding</code></strong>=<em><code>1e-05</code></em>, <strong><code>l2_reg_dnn</code></strong>=<em><code>0</code></em>, <strong><code>init_std</code></strong>=<em><code>0.0001</code></em>, <strong><code>seed</code></strong>=<em><code>1024</code></em>, <strong><code>device</code></strong>=<em><code>'cpu'</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">log_loss</span><span class="p">,</span> <span class="n">roc_auc_score</span>

<span class="kn">from</span> <span class="nn">model</span> <span class="kn">import</span> <span class="n">DeepFMv2</span> <span class="k">as</span> <span class="n">DeepFM</span>
<span class="kn">from</span> <span class="nn">recohut.datasets.criteo</span> <span class="kn">import</span> <span class="n">CriteoSampleDataset</span>


<span class="k">def</span> <span class="nf">get_auc</span><span class="p">(</span><span class="n">loader</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
    <span class="n">pred</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">loader</span><span class="p">:</span>
            <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
            <span class="n">y_hat</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">pred</span> <span class="o">+=</span> <span class="nb">list</span><span class="p">(</span><span class="n">y_hat</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
            <span class="n">target</span> <span class="o">+=</span> <span class="nb">list</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
    <span class="n">auc</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">pred</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">auc</span>


<span class="n">root</span> <span class="o">=</span> <span class="s1">&#39;/content/data&#39;</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1024</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">seed</span> <span class="o">=</span> <span class="mi">1024</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.00005</span>
<span class="n">wd</span> <span class="o">=</span> <span class="mf">0.00001</span>
<span class="n">device</span> <span class="o">=</span> <span class="s1">&#39;cpu&#39;</span>

<span class="n">ds</span> <span class="o">=</span> <span class="n">CriteoSampleDataset</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="n">root</span><span class="p">)</span>
<span class="n">train_tensor_data</span><span class="p">,</span> <span class="n">test_tensor_data</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">load</span><span class="p">()</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_tensor_data</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">test_tensor_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>

<span class="n">sparse_features</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;C&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">27</span><span class="p">)]</span>
<span class="n">dense_features</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;I&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">14</span><span class="p">)]</span>

<span class="c1"># model = NFM(ds.feat_sizes, embedding_size, ds.linear_feature_columns, ds.dnn_feature_columns).to(device)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DeepFMv2</span><span class="p">(</span><span class="n">ds</span><span class="o">.</span><span class="n">feat_sizes</span><span class="p">,</span> <span class="n">sparse_feature_columns</span><span class="o">=</span><span class="n">sparse_features</span><span class="p">,</span> <span class="n">dense_feature_columns</span><span class="o">=</span><span class="n">dense_features</span><span class="p">,</span>
                <span class="n">dnn_hidden_units</span><span class="o">=</span><span class="p">[</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">250</span><span class="p">],</span> <span class="n">dnn_dropout</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">ebedding_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
                <span class="n">l2_reg_linear</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">loss_func</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">wd</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">total_loss_epoch</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">total_tmp</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">total_loss_epoch</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">total_tmp</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">auc</span> <span class="o">=</span> <span class="n">get_auc</span><span class="p">(</span><span class="n">test_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;epoch/epoches: </span><span class="si">{}</span><span class="s1">/</span><span class="si">{}</span><span class="s1">, train loss: </span><span class="si">{:.3f}</span><span class="s1">, test auc: </span><span class="si">{:.3f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">total_loss_epoch</span> <span class="o">/</span> <span class="n">total_tmp</span><span class="p">,</span> <span class="n">auc</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>epoch/epoches: 0/10, train loss: 0.570, test auc: 0.684
epoch/epoches: 1/10, train loss: 0.534, test auc: 0.714
epoch/epoches: 2/10, train loss: 0.511, test auc: 0.725
epoch/epoches: 3/10, train loss: 0.486, test auc: 0.732
epoch/epoches: 4/10, train loss: 0.459, test auc: 0.738
epoch/epoches: 5/10, train loss: 0.431, test auc: 0.743
epoch/epoches: 6/10, train loss: 0.401, test auc: 0.743
epoch/epoches: 7/10, train loss: 0.368, test auc: 0.740
epoch/epoches: 8/10, train loss: 0.337, test auc: 0.735
epoch/epoches: 9/10, train loss: 0.312, test auc: 0.729
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#     def __init__(self, n_users, n_items, embedding_dim, batch_norm=True, dropout=0.1, num_layers=3, act_function=&#39;relu&#39;):</span>
<span class="c1">#         &quot;&quot;&quot;</span>
<span class="c1">#         Args:</span>
<span class="c1">#             n_users : int, the number of users</span>
<span class="c1">#             n_items : int, the number of items</span>
<span class="c1">#             embedding_dim : int, the number of latent factoact_function : str, activation function for hidden layer</span>
<span class="c1">#             num_layers : int, number of hidden layers</span>
<span class="c1">#             batch_norm : bool, whether to normalize a batch of data</span>
<span class="c1">#             dropout : float, dropout rate</span>
<span class="c1">#         &quot;&quot;&quot;</span>
<span class="c1">#         super().__init__()</span>

<span class="c1">#         self.num_layers = num_layers</span>

<span class="c1">#         self.user_embedding = nn.Embedding(</span>
<span class="c1">#             num_embeddings=n_users, embedding_dim=embedding_dim</span>
<span class="c1">#         )</span>
<span class="c1">#         self.item_embedding = nn.Embedding(</span>
<span class="c1">#             num_embeddings=n_items, embedding_dim=embedding_dim</span>
<span class="c1">#         )</span>
<span class="c1">#         self.user_bias = nn.Embedding(n_users, 1)</span>
<span class="c1">#         self.item_bias = nn.Embedding(n_items, 1)</span>
<span class="c1">#         self.bias_ = nn.Parameter(torch.tensor([0.0]))</span>

<span class="c1">#         fm_modules = []</span>
<span class="c1">#         if batch_norm:</span>
<span class="c1">#             fm_modules.append(nn.BatchNorm1d(embedding_dim))</span>
<span class="c1">#         fm_modules.append(nn.Dropout(dropout))</span>
<span class="c1">#         self.fm_layers = nn.Sequential(*fm_modules)</span>

<span class="c1">#         deep_modules = []</span>
<span class="c1">#         in_dim = embedding_dim * 2   # user &amp; item</span>
<span class="c1">#         for _ in range(num_layers):  # _ is dim if layers is list</span>
<span class="c1">#             out_dim = in_dim</span>
<span class="c1">#             deep_modules.append(nn.Linear(in_dim, out_dim))</span>
<span class="c1">#             in_dim = out_dim</span>
<span class="c1">#             if batch_norm:</span>
<span class="c1">#                 deep_modules.append(nn.BatchNorm1d(out_dim))</span>
<span class="c1">#             if act_function == &#39;relu&#39;:</span>
<span class="c1">#                 deep_modules.append(nn.ReLU())</span>
<span class="c1">#             elif act_function == &#39;sigmoid&#39;:</span>
<span class="c1">#                 deep_modules.append(nn.Sigmoid())</span>
<span class="c1">#             elif act_function == &#39;tanh&#39;:</span>
<span class="c1">#                 deep_modules.append(nn.Tanh())</span>
<span class="c1">#             deep_modules.append(nn.Dropout(dropout))</span>

<span class="c1">#         self.deep_layers = nn.Sequential(*deep_modules)</span>
<span class="c1">#         self.deep_out = nn.Linear(in_dim, 1, bias=False)</span>

<span class="c1">#         self._init_weights()</span>

<span class="c1">#     def _init_weights(self):</span>
<span class="c1">#         nn.init.normal_(self.item_embedding.weight, std=0.01)</span>
<span class="c1">#         nn.init.normal_(self.user_embedding.weight, std=0.01)</span>
<span class="c1">#         nn.init.constant_(self.user_bias.weight, 0.0)</span>
<span class="c1">#         nn.init.constant_(self.item_bias.weight, 0.0)</span>

<span class="c1">#         # for deep layers</span>
<span class="c1">#         for m in self.deep_layers:</span>
<span class="c1">#             if isinstance(m, nn.Linear):</span>
<span class="c1">#                 nn.init.xavier_normal_(m.weight)</span>
<span class="c1">#         nn.init.xavier_normal_(self.deep_out.weight)</span>

<span class="c1">#     def forward(self, users, items):</span>
<span class="c1">#         embed_user = self.user_embedding(users)</span>
<span class="c1">#         embed_item = self.item_embedding(items)</span>

<span class="c1">#         fm = embed_user * embed_item</span>
<span class="c1">#         fm = self.fm_layers(fm)</span>
<span class="c1">#         y_fm = fm.sum(dim=-1)</span>

<span class="c1">#         y_fm = y_fm + self.user_bias(users) + self.item_bias(items) + self.bias_</span>

<span class="c1">#         if self.num_layers:</span>
<span class="c1">#             fm = self.deep_layers(fm)</span>

<span class="c1">#         y_deep = torch.cat((embed_user, embed_item), dim=-1)</span>
<span class="c1">#         y_deep = self.deep_layers(y_deep)</span>

<span class="c1">#         # since BCELoss will automatically transfer pred with sigmoid</span>
<span class="c1">#         # there is no need to use extra nn.Sigmoid(pred)</span>
<span class="c1">#         pred = y_fm + y_deep</span>

<span class="c1">#         return pred.view(-1)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

</div>
 

