---

title: Deep and Cross Network (DCN)


keywords: fastai
sidebar: home_sidebar



nb_path: "nbs/models/tf/dcn.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/models/tf/dcn.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="CrossNetwork" class="doc_header"><code>class</code> <code>CrossNetwork</code><a href="https://github.com/RecoHut-Projects/recohut/tree/master/recohut/models/tf/dcn.py#L15" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>CrossNetwork</code>(<strong>*<code>args</code></strong>, <strong>**<code>kwargs</code></strong>) :: <code>Layer</code></p>
</blockquote>
<p>This is the class from which all layers inherit.</p>
<p>A layer is a callable object that takes as input one or more tensors and
that outputs one or more tensors. It involves <em>computation</em>, defined
in the <code>call()</code> method, and a <em>state</em> (weight variables), defined
either in the constructor <code>__init__()</code> or in the <code>build()</code> method.</p>
<p>Users will just instantiate a layer and then treat it as a callable.</p>
<p>Args:
  trainable: Boolean, whether the layer's variables should be trainable.
  name: String name of the layer.
  dtype: The dtype of the layer's computations and weights. Can also be a
    <code>tf.keras.mixed_precision.Policy</code>, which allows the computation and weight
    dtype to differ. Default of <code>None</code> means to use
    <code>tf.keras.mixed_precision.global_policy()</code>, which is a float32 policy
    unless set to different value.
  dynamic: Set this to <code>True</code> if your layer should only be run eagerly, and
    should not be used to generate a static computation graph.
    This would be the case for a Tree-RNN or a recursive network,
    for example, or generally for any layer that manipulates tensors
    using Python control flow. If <code>False</code>, we assume that the layer can
    safely be used to generate a static computation graph.</p>
<p>Attributes:
  name: The name of the layer (string).
  dtype: The dtype of the layer's weights.
  variable_dtype: Alias of <code>dtype</code>.
  compute_dtype: The dtype of the layer's computations. Layers automatically
    cast inputs to this dtype which causes the computations and output to also
    be in this dtype. When mixed precision is used with a
    <code>tf.keras.mixed_precision.Policy</code>, this will be different than
    <code>variable_dtype</code>.
  dtype_policy: The layer's dtype policy. See the
    <code>tf.keras.mixed_precision.Policy</code> documentation for details.
  trainable_weights: List of variables to be included in backprop.
  non_trainable_weights: List of variables that should not be
    included in backprop.
  weights: The concatenation of the lists trainable_weights and
    non_trainable_weights (in this order).
  trainable: Whether the layer should be trained (boolean), i.e. whether
    its potentially-trainable weights should be returned as part of
    <code>layer.trainable_weights</code>.
  input_spec: Optional (list of) <code>InputSpec</code> object(s) specifying the
    constraints on inputs that can be accepted by the layer.</p>
<p>We recommend that descendants of <code>Layer</code> implement the following methods:</p>
<ul>
<li><code>__init__()</code>: Defines custom layer attributes, and creates layer state
variables that do not depend on input shapes, using <code>add_weight()</code>.</li>
<li><code>build(self, input_shape)</code>: This method can be used to create weights that
depend on the shape(s) of the input(s), using <code>add_weight()</code>. <code>__call__()</code>
will automatically build the layer (if it has not been built yet) by
calling <code>build()</code>.</li>
<li><code>call(self, inputs, *args, **kwargs)</code>: Called in <code>__call__</code> after making
sure <code>build()</code> has been called. <code>call()</code> performs the logic of applying the
layer to the input tensors (which should be passed in as argument).
Two reserved keyword arguments you can optionally use in <code>call()</code> are:<ul>
<li><code>training</code> (boolean, whether the call is in inference mode or training
mode). See more details in <a href="https://www.tensorflow.org/guide/keras/custom_layers_and_models#privileged_training_argument_in_the_call_method">the layer/model subclassing guide</a></li>
<li><code>mask</code> (boolean tensor encoding masked timesteps in the input, used
in RNN layers). See more details in <a href="https://www.tensorflow.org/guide/keras/custom_layers_and_models#privileged_mask_argument_in_the_call_method">the layer/model subclassing guide</a>
A typical signature for this method is <code>call(self, inputs)</code>, and user could
optionally add <code>training</code> and <code>mask</code> if the layer need them. <code>*args</code> and
<code>**kwargs</code> is only useful for future extension when more input parameters
are planned to be added.</li>
</ul>
</li>
<li><code>get_config(self)</code>: Returns a dictionary containing the configuration used
to initialize this layer. If the keys differ from the arguments
in <code>__init__</code>, then override <code>from_config(self)</code> as well.
This method is used when saving
the layer or a model that contains this layer.</li>
</ul>
<p>Examples:</p>
<p>Here's a basic example: a layer with two variables, <code>w</code> and <code>b</code>,
that returns <code>y = w . x + b</code>.
It shows how to implement <code>build()</code> and <code>call()</code>.
Variables set as attributes of a layer are tracked as weights
of the layers (in <code>layer.weights</code>).</p>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SimpleDense</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">units</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
      <span class="nb">super</span><span class="p">(</span><span class="n">SimpleDense</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">units</span> <span class="o">=</span> <span class="n">units</span>

  <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>  <span class="c1"># Create the state of the layer (weights)</span>
    <span class="n">w_init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random_normal_initializer</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span>
        <span class="n">initial_value</span><span class="o">=</span><span class="n">w_init</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">units</span><span class="p">),</span>
                             <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float32&#39;</span><span class="p">),</span>
        <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">b_init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span>
        <span class="n">initial_value</span><span class="o">=</span><span class="n">b_init</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">units</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float32&#39;</span><span class="p">),</span>
        <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>  <span class="c1"># Defines the computation from inputs to outputs</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span>

<span class="c1"># Instantiates the layer.</span>
<span class="n">linear_layer</span> <span class="o">=</span> <span class="n">SimpleDense</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>

<span class="c1"># This will also call `build(input_shape)` and create the weights.</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">linear_layer</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
<span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">linear_layer</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>

<span class="c1"># These weights are trainable, so they&#39;re listed in `trainable_weights`:</span>
<span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">linear_layer</span><span class="o">.</span><span class="n">trainable_weights</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
</pre></div>
<p>Note that the method <code>add_weight()</code> offers a shortcut to create weights:</p>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SimpleDense</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">units</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
      <span class="nb">super</span><span class="p">(</span><span class="n">SimpleDense</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">units</span> <span class="o">=</span> <span class="n">units</span>

  <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">units</span><span class="p">),</span>
                               <span class="n">initializer</span><span class="o">=</span><span class="s1">&#39;random_normal&#39;</span><span class="p">,</span>
                               <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">units</span><span class="p">,),</span>
                               <span class="n">initializer</span><span class="o">=</span><span class="s1">&#39;random_normal&#39;</span><span class="p">,</span>
                               <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span>
</pre></div>
<p>Besides trainable weights, updated via backpropagation during training,
layers can also have non-trainable weights. These weights are meant to
be updated manually during <code>call()</code>. Here's a example layer that computes
the running sum of its inputs:</p>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ComputeSum</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">):</span>
      <span class="nb">super</span><span class="p">(</span><span class="n">ComputeSum</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
      <span class="c1"># Create a non-trainable weight.</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">total</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">initial_value</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">input_dim</span><span class="p">,)),</span>
                               <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">total</span><span class="o">.</span><span class="n">assign_add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">total</span>

<span class="n">my_sum</span> <span class="o">=</span> <span class="n">ComputeSum</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">my_sum</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>  <span class="c1"># [2. 2.]</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">my_sum</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>  <span class="c1"># [4. 4.]</span>

<span class="k">assert</span> <span class="n">my_sum</span><span class="o">.</span><span class="n">weights</span> <span class="o">==</span> <span class="p">[</span><span class="n">my_sum</span><span class="o">.</span><span class="n">total</span><span class="p">]</span>
<span class="k">assert</span> <span class="n">my_sum</span><span class="o">.</span><span class="n">non_trainable_weights</span> <span class="o">==</span> <span class="p">[</span><span class="n">my_sum</span><span class="o">.</span><span class="n">total</span><span class="p">]</span>
<span class="k">assert</span> <span class="n">my_sum</span><span class="o">.</span><span class="n">trainable_weights</span> <span class="o">==</span> <span class="p">[]</span>
</pre></div>
<p>For more information about creating layers, see the guide
<a href="https://www.tensorflow.org/guide/keras/custom_layers_and_models">Making new Layers and Models via subclassing</a></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="DNN" class="doc_header"><code>class</code> <code>DNN</code><a href="https://github.com/RecoHut-Projects/recohut/tree/master/recohut/models/tf/dcn.py#L56" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>DNN</code>(<strong>*<code>args</code></strong>, <strong>**<code>kwargs</code></strong>) :: <code>Layer</code></p>
</blockquote>
<p>This is the class from which all layers inherit.</p>
<p>A layer is a callable object that takes as input one or more tensors and
that outputs one or more tensors. It involves <em>computation</em>, defined
in the <code>call()</code> method, and a <em>state</em> (weight variables), defined
either in the constructor <code>__init__()</code> or in the <code>build()</code> method.</p>
<p>Users will just instantiate a layer and then treat it as a callable.</p>
<p>Args:
  trainable: Boolean, whether the layer's variables should be trainable.
  name: String name of the layer.
  dtype: The dtype of the layer's computations and weights. Can also be a
    <code>tf.keras.mixed_precision.Policy</code>, which allows the computation and weight
    dtype to differ. Default of <code>None</code> means to use
    <code>tf.keras.mixed_precision.global_policy()</code>, which is a float32 policy
    unless set to different value.
  dynamic: Set this to <code>True</code> if your layer should only be run eagerly, and
    should not be used to generate a static computation graph.
    This would be the case for a Tree-RNN or a recursive network,
    for example, or generally for any layer that manipulates tensors
    using Python control flow. If <code>False</code>, we assume that the layer can
    safely be used to generate a static computation graph.</p>
<p>Attributes:
  name: The name of the layer (string).
  dtype: The dtype of the layer's weights.
  variable_dtype: Alias of <code>dtype</code>.
  compute_dtype: The dtype of the layer's computations. Layers automatically
    cast inputs to this dtype which causes the computations and output to also
    be in this dtype. When mixed precision is used with a
    <code>tf.keras.mixed_precision.Policy</code>, this will be different than
    <code>variable_dtype</code>.
  dtype_policy: The layer's dtype policy. See the
    <code>tf.keras.mixed_precision.Policy</code> documentation for details.
  trainable_weights: List of variables to be included in backprop.
  non_trainable_weights: List of variables that should not be
    included in backprop.
  weights: The concatenation of the lists trainable_weights and
    non_trainable_weights (in this order).
  trainable: Whether the layer should be trained (boolean), i.e. whether
    its potentially-trainable weights should be returned as part of
    <code>layer.trainable_weights</code>.
  input_spec: Optional (list of) <code>InputSpec</code> object(s) specifying the
    constraints on inputs that can be accepted by the layer.</p>
<p>We recommend that descendants of <code>Layer</code> implement the following methods:</p>
<ul>
<li><code>__init__()</code>: Defines custom layer attributes, and creates layer state
variables that do not depend on input shapes, using <code>add_weight()</code>.</li>
<li><code>build(self, input_shape)</code>: This method can be used to create weights that
depend on the shape(s) of the input(s), using <code>add_weight()</code>. <code>__call__()</code>
will automatically build the layer (if it has not been built yet) by
calling <code>build()</code>.</li>
<li><code>call(self, inputs, *args, **kwargs)</code>: Called in <code>__call__</code> after making
sure <code>build()</code> has been called. <code>call()</code> performs the logic of applying the
layer to the input tensors (which should be passed in as argument).
Two reserved keyword arguments you can optionally use in <code>call()</code> are:<ul>
<li><code>training</code> (boolean, whether the call is in inference mode or training
mode). See more details in <a href="https://www.tensorflow.org/guide/keras/custom_layers_and_models#privileged_training_argument_in_the_call_method">the layer/model subclassing guide</a></li>
<li><code>mask</code> (boolean tensor encoding masked timesteps in the input, used
in RNN layers). See more details in <a href="https://www.tensorflow.org/guide/keras/custom_layers_and_models#privileged_mask_argument_in_the_call_method">the layer/model subclassing guide</a>
A typical signature for this method is <code>call(self, inputs)</code>, and user could
optionally add <code>training</code> and <code>mask</code> if the layer need them. <code>*args</code> and
<code>**kwargs</code> is only useful for future extension when more input parameters
are planned to be added.</li>
</ul>
</li>
<li><code>get_config(self)</code>: Returns a dictionary containing the configuration used
to initialize this layer. If the keys differ from the arguments
in <code>__init__</code>, then override <code>from_config(self)</code> as well.
This method is used when saving
the layer or a model that contains this layer.</li>
</ul>
<p>Examples:</p>
<p>Here's a basic example: a layer with two variables, <code>w</code> and <code>b</code>,
that returns <code>y = w . x + b</code>.
It shows how to implement <code>build()</code> and <code>call()</code>.
Variables set as attributes of a layer are tracked as weights
of the layers (in <code>layer.weights</code>).</p>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SimpleDense</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">units</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
      <span class="nb">super</span><span class="p">(</span><span class="n">SimpleDense</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">units</span> <span class="o">=</span> <span class="n">units</span>

  <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>  <span class="c1"># Create the state of the layer (weights)</span>
    <span class="n">w_init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random_normal_initializer</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span>
        <span class="n">initial_value</span><span class="o">=</span><span class="n">w_init</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">units</span><span class="p">),</span>
                             <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float32&#39;</span><span class="p">),</span>
        <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">b_init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span>
        <span class="n">initial_value</span><span class="o">=</span><span class="n">b_init</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">units</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float32&#39;</span><span class="p">),</span>
        <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>  <span class="c1"># Defines the computation from inputs to outputs</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span>

<span class="c1"># Instantiates the layer.</span>
<span class="n">linear_layer</span> <span class="o">=</span> <span class="n">SimpleDense</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>

<span class="c1"># This will also call `build(input_shape)` and create the weights.</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">linear_layer</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
<span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">linear_layer</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>

<span class="c1"># These weights are trainable, so they&#39;re listed in `trainable_weights`:</span>
<span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">linear_layer</span><span class="o">.</span><span class="n">trainable_weights</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
</pre></div>
<p>Note that the method <code>add_weight()</code> offers a shortcut to create weights:</p>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SimpleDense</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">units</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
      <span class="nb">super</span><span class="p">(</span><span class="n">SimpleDense</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">units</span> <span class="o">=</span> <span class="n">units</span>

  <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">units</span><span class="p">),</span>
                               <span class="n">initializer</span><span class="o">=</span><span class="s1">&#39;random_normal&#39;</span><span class="p">,</span>
                               <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">units</span><span class="p">,),</span>
                               <span class="n">initializer</span><span class="o">=</span><span class="s1">&#39;random_normal&#39;</span><span class="p">,</span>
                               <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span>
</pre></div>
<p>Besides trainable weights, updated via backpropagation during training,
layers can also have non-trainable weights. These weights are meant to
be updated manually during <code>call()</code>. Here's a example layer that computes
the running sum of its inputs:</p>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ComputeSum</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">):</span>
      <span class="nb">super</span><span class="p">(</span><span class="n">ComputeSum</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
      <span class="c1"># Create a non-trainable weight.</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">total</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">initial_value</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">input_dim</span><span class="p">,)),</span>
                               <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">total</span><span class="o">.</span><span class="n">assign_add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">total</span>

<span class="n">my_sum</span> <span class="o">=</span> <span class="n">ComputeSum</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">my_sum</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>  <span class="c1"># [2. 2.]</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">my_sum</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>  <span class="c1"># [4. 4.]</span>

<span class="k">assert</span> <span class="n">my_sum</span><span class="o">.</span><span class="n">weights</span> <span class="o">==</span> <span class="p">[</span><span class="n">my_sum</span><span class="o">.</span><span class="n">total</span><span class="p">]</span>
<span class="k">assert</span> <span class="n">my_sum</span><span class="o">.</span><span class="n">non_trainable_weights</span> <span class="o">==</span> <span class="p">[</span><span class="n">my_sum</span><span class="o">.</span><span class="n">total</span><span class="p">]</span>
<span class="k">assert</span> <span class="n">my_sum</span><span class="o">.</span><span class="n">trainable_weights</span> <span class="o">==</span> <span class="p">[]</span>
</pre></div>
<p>For more information about creating layers, see the guide
<a href="https://www.tensorflow.org/guide/keras/custom_layers_and_models">Making new Layers and Models via subclassing</a></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="DCN" class="doc_header"><code>class</code> <code>DCN</code><a href="https://github.com/RecoHut-Projects/recohut/tree/master/recohut/models/tf/dcn.py#L75" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>DCN</code>(<strong>*<code>args</code></strong>, <strong>**<code>kwargs</code></strong>) :: <code>Model</code></p>
</blockquote>
<p><code>Model</code> groups layers into an object with training and inference features.</p>
<p>Args:
    inputs: The input(s) of the model: a <code>keras.Input</code> object or list of
        <code>keras.Input</code> objects.
    outputs: The output(s) of the model. See Functional API example below.
    name: String, the name of the model.</p>
<p>There are two ways to instantiate a <code>Model</code>:</p>
<p>1 - With the "Functional API", where you start from <code>Input</code>,
you chain layer calls to specify the model's forward pass,
and finally you create your model from inputs and outputs:</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>
</pre></div>
<p>Note: Only dicts, lists, and tuples of input tensors are supported. Nested
inputs are not supported (e.g. lists of list or dicts of dict).</p>
<p>A new Functional API model can also be created by using the
intermediate tensors. This enables you to quickly extract sub-components
of the model.</p>
<p>Example:</p>
<div class="highlight"><pre><span></span><span class="n">inputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">processed</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">RandomCrop</span><span class="p">(</span><span class="n">width</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">32</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">conv</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">)(</span><span class="n">processed</span><span class="p">)</span>
<span class="n">pooling</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">GlobalAveragePooling2D</span><span class="p">()(</span><span class="n">conv</span><span class="p">)</span>
<span class="n">feature</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">)(</span><span class="n">pooling</span><span class="p">)</span>

<span class="n">full_model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">feature</span><span class="p">)</span>
<span class="n">backbone</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">processed</span><span class="p">,</span> <span class="n">conv</span><span class="p">)</span>
<span class="n">activations</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">conv</span><span class="p">,</span> <span class="n">feature</span><span class="p">)</span>
</pre></div>
<p>Note that the <code>backbone</code> and <code>activations</code> models are not
created with <code>keras.Input</code> objects, but with the tensors that are originated
from <code>keras.Inputs</code> objects. Under the hood, the layers and weights will
be shared across these models, so that user can train the <code>full_model</code>, and
use <code>backbone</code> or <code>activations</code> to do feature extraction.
The inputs and outputs of the model can be nested structures of tensors as
well, and the created models are standard Functional API models that support
all the existing APIs.</p>
<p>2 - By subclassing the <code>Model</code> class: in that case, you should define your
layers in <code>__init__()</code> and you should implement the model's forward pass
in <code>call()</code>.</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="k">class</span> <span class="nc">MyModel</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dense1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dense2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense1</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span>
</pre></div>
<p>If you subclass <code>Model</code>, you can optionally have
a <code>training</code> argument (boolean) in <code>call()</code>, which you can use to specify
a different behavior in training and inference:</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="k">class</span> <span class="nc">MyModel</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dense1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dense2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense1</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">training</span><span class="p">:</span>
      <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span>
</pre></div>
<p>Once the model is created, you can config the model with losses and metrics
with <code>model.compile()</code>, train the model with <code>model.fit()</code>, or use the model
to do prediction with <code>model.predict()</code>.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">test_model</span><span class="p">():</span>
    <span class="n">user_features</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;feat&#39;</span><span class="p">:</span> <span class="s1">&#39;user_id&#39;</span><span class="p">,</span> <span class="s1">&#39;feat_num&#39;</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span> <span class="s1">&#39;embed_dim&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">}</span>
    <span class="n">seq_features</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;feat&#39;</span><span class="p">:</span> <span class="s1">&#39;item_id&#39;</span><span class="p">,</span> <span class="s1">&#39;feat_num&#39;</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span> <span class="s1">&#39;embed_dim&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">}</span>
    <span class="n">features</span> <span class="o">=</span> <span class="p">[</span><span class="n">user_features</span><span class="p">,</span> <span class="n">seq_features</span><span class="p">]</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">DCN</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">hidden_units</span><span class="o">=</span><span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">dnn_dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">test_model</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Model: &#34;model_2&#34;
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 tf.__operators__.getitem_4 (Sl  (None,)             0           [&#39;input_3[0][0]&#39;]                
 icingOpLambda)                                                                                   
                                                                                                  
 tf.__operators__.getitem_5 (Sl  (None,)             0           [&#39;input_3[0][0]&#39;]                
 icingOpLambda)                                                                                   
                                                                                                  
 embedding_8 (Embedding)        (None, 8)            800         [&#39;tf.__operators__.getitem_4[0][0
                                                                 ]&#39;]                              
                                                                                                  
 embedding_9 (Embedding)        (None, 8)            800         [&#39;tf.__operators__.getitem_5[0][0
                                                                 ]&#39;]                              
                                                                                                  
 tf.concat_1 (TFOpLambda)       (None, 16)           0           [&#39;embedding_8[0][0]&#39;,            
                                                                  &#39;embedding_9[0][0]&#39;]            
                                                                                                  
 cross_network (CrossNetwork)   (None, 16)           96          [&#39;tf.concat_1[0][0]&#39;]            
                                                                                                  
 dnn_1 (DNN)                    (None, 2)            182         [&#39;tf.concat_1[0][0]&#39;]            
                                                                                                  
 tf.concat_2 (TFOpLambda)       (None, 18)           0           [&#39;cross_network[0][0]&#39;,          
                                                                  &#39;dnn_1[0][0]&#39;]                  
                                                                                                  
 dense_13 (Dense)               (None, 1)            19          [&#39;tf.concat_2[0][0]&#39;]            
                                                                                                  
 tf.math.sigmoid_2 (TFOpLambda)  (None, 1)           0           [&#39;dense_13[0][0]&#39;]               
                                                                                                  
==================================================================================================
Total params: 1,897
Trainable params: 1,897
Non-trainable params: 0
__________________________________________________________________________________________________
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

