{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c3aaa2d-6b9c-4bd9-a1b8-215eac181193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "DQN SETTINGS:\n",
      "--------------------\n",
      "State dim: 4\n",
      "Action dim: 1\n",
      "Input type: array\n",
      "Seed: 1\n",
      "Device: cpu\n",
      "\n",
      "Hyperparameters:\n",
      "--------------------\n",
      "Algorithm type: rainbow\n",
      "Hidden dimensions: 16\n",
      "Batch size: 32\n",
      "Discount factor: 0.95\n",
      "Learning rate: 0.0005\n",
      "Steps per network update: 1\n",
      "Target Update Method: soft\n",
      "Soft target update factor: 0.01\n",
      "Replay type: default\n",
      "Exploration Method: noisy_network\n",
      "Categorical Learning: True\n",
      "Value function range: (0, 200)\n",
      "Categorical atom size: 51\n",
      "Multi-step learning: 4\n",
      "--------------------\n",
      "\n",
      "Ep 10 - Mean Reward 20.3\n",
      "Ep 20 - Mean Reward 14.8\n",
      "Ep 30 - Mean Reward 18.9\n",
      "Ep 40 - Mean Reward 25.8\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from AgentRL.agents import DQN\n",
    "from AgentRL.common.buffers import prioritised_replay_buffer, standard_replay_buffer\n",
    "\n",
    "import torch\n",
    "\n",
    "def train(env):\n",
    "\n",
    "    # Set the hyperparameters\n",
    "    training = True\n",
    "    render = False\n",
    "    display_freq = 10\n",
    "    episodes = 1000\n",
    "    timestep_limit = 100\n",
    "    seed = 1\n",
    "    \n",
    "    # intialise the environment\n",
    "    env = env\n",
    "    env.seed(seed)\n",
    "\n",
    "    running_reward = []\n",
    "\n",
    "    # initialise the agent\n",
    "    # buffer = prioritised_replay_buffer(max_size=50_000, seed=seed)\n",
    "    buffer = standard_replay_buffer(max_size=50_000, seed=seed)\n",
    "    agent = DQN(\n",
    "        state_dim=env.observation_space.shape[0], \n",
    "        action_num=env.action_space.n, \n",
    "        replay_buffer=buffer,\n",
    "\n",
    "        algorithm_type='rainbow',\n",
    "        hidden_dim = 16,\n",
    "        learning_rate = 5e-4,\n",
    "        batch_size = 32,\n",
    "        gamma = 0.95,\n",
    "        \n",
    "        target_update_method = 'soft',\n",
    "        tau = 0.01, # for soft\n",
    "        target_update_freq = 20, # for hard\n",
    "        \n",
    "        # exploration_method=\"greedy\",\n",
    "                \n",
    "        categorical = True,\n",
    "        v_range = (0, 200),\n",
    "        atom_size = 51,\n",
    "        \n",
    "        multi_step = 4,\n",
    "\n",
    "        seed = seed\n",
    "\n",
    "    )\n",
    "\n",
    "    for ep in range(1, episodes + 1):\n",
    "\n",
    "        # reset the state\n",
    "        state, done = env.reset(), False\n",
    "        counter = 0\n",
    "        episode_reward = 0\n",
    "\n",
    "        # run the training loop\n",
    "        while not done:\n",
    "\n",
    "            action = agent.get_action(state=state.flatten())              \n",
    "            next_state, reward, done, info = env.step(action=action[0])\n",
    "\n",
    "            # render the environment\n",
    "            if render: \n",
    "                env.render(mode='close')\n",
    "\n",
    "            # update the reward total\n",
    "            episode_reward += reward\n",
    "\n",
    "\n",
    "            if training: \n",
    "\n",
    "                # push test samples to the replay buffer\n",
    "                agent.push(state=state, action=action,\n",
    "                            next_state=next_state, reward=reward/100, done=done)\n",
    "\n",
    "                agent.update()                       \n",
    "\n",
    "            # update the state\n",
    "            state = next_state\n",
    "            counter += 1\n",
    "\n",
    "            # terminate when episode limit is reached            \n",
    "            if counter >= timestep_limit:\n",
    "                done = True\n",
    "\n",
    "            # print the episode reward\n",
    "            if done: \n",
    "\n",
    "                # get reward mean\n",
    "                running_reward.append(episode_reward)\n",
    "\n",
    "                if ep % display_freq == 0:\n",
    "                    # print('Ep {} - Mean Reward {} Exploration {}'.format(ep, sum(running_reward) / display_freq, round(agent.policy.current_exploration, 2)))\n",
    "                    print('Ep {} - Mean Reward {}'.format(ep, sum(running_reward) / display_freq))\n",
    "                    running_reward = []  \n",
    "\n",
    "    # close the display\n",
    "    env.close()    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # get the environment    \n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    \n",
    "    # run the program\n",
    "    try: \n",
    "        train(env)\n",
    "    \n",
    "    # shut the y window if interrupted\n",
    "    except KeyboardInterrupt:\n",
    "        env.close()        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
