# -*- coding: utf-8 -*-
"""alfredyang@pharbers.com.

This is job template for Pharbers Max Job
"""
import json
import click
import traceback
import requests
import re
import boto3

from phjob import execute
from phcli.ph_logs.ph_logs import phs3logger, LOG_DEBUG_LEVEL
from phcli.ph_max_auto.ph_hook.ph_hook import exec_before, exec_after
from pyspark.sql.functions import lit

def create_df(input_ds, spark, output_version, project_id, logger):

    def create_clickhouse_df(name, version):

        df = spark.read.format("jdbc") \
            .option("url", "jdbc:clickhouse://192.168.16.117:8123") \
            .option("dbtable", project_id + "_" + name) \
            .option("driver", "ru.yandex.clickhouse.ClickHouseDriver") \
            .option("user", "default") \
            .option("password", "") \
            .option("fetchsize", "500000").load()
        df = df.where(df["version"].isin(version))
        return df


    def create_s3_df(path):

        if path.endswith(".csv"):
            df = spark.read.csv(path, header=True)
        else:
            df = spark.read.parquet(path)

        return df

    def lowerColumns(df):
        df = df.toDF(*[i.lower() for i in df.columns])
        return df

    version = input_ds.get("version")
    if len(version) == 0:
        version.append(output_version)
    name = input_ds.get("name")

    if input_ds.get("cat") == "intermediate" or input_ds.get("cat") == "uploaded":
        df = create_clickhouse_df(name, version)
    elif input_ds.get("cat") == "input_index":
        prop = input_ds.get("prop")
        path = prop.get("path")
        df = create_s3_df(path)

    df = lowerColumns(df)

    return df


def readClickhouse(inputs, kwargs, project_id, output_version, logger):
    df_map = {}
    spark = kwargs['spark']()

    ds_list = kwargs.get("ds_conf")
    name_ds_map_dict = {}
    for ds in ds_list:
        name_ds_map_dict.update({ds.get("name"): ds})
    logger.debug("name_ds_map_dict")
    logger.debug(name_ds_map_dict)
    for input_ds_name in inputs:
        if input_ds_name in name_ds_map_dict.keys():
            input_df = create_df(name_ds_map_dict.get(input_ds_name), spark, output_version, project_id, logger)
            df_map.update({"df_" +input_ds_name: input_df})
        else:
            input_ds = {
                "name": input_ds_name,
                "version": output_version.split(","),
                "cat": "intermediate"
            }
            input_df = create_df(input_ds, spark, output_version, project_id, logger)
            df_map.update({"df_" +input_ds_name: input_df})
    return df_map

def create_prepare_df(inputs, kwargs, project_id, output_version, logger):

    spark = kwargs['spark']()

    ds_list = kwargs.get("ds_conf")
    name_ds_map_dict = {}
    for ds in ds_list:
        name_ds_map_dict.update({ds.get("name"): ds})
    for input_ds_name in inputs:
        if input_ds_name in name_ds_map_dict.keys():
            input_df = create_df(name_ds_map_dict.get(input_ds_name), spark, output_version, project_id, logger)

        else:
            input_ds = {
                "name": input_ds_name,
                "version": output_version.split(","),
                "cat": "intermediate"
            }
            input_df = create_df(input_ds, spark, output_version, project_id, logger)

    return {"input_df": input_df}

def create_input_df(runtime, inputs, args, project_id, output_version, logger):
    df_choose = {
        "pyspark": readClickhouse(inputs, args, project_id, output_version, logger),
        "python3": readClickhouse(inputs, args, project_id, output_version, logger),
        "r": readClickhouse(inputs, args, project_id, output_version, logger),
        "sparkr": readClickhouse(inputs, args, project_id, output_version, logger),
        "prepare": create_prepare_df(inputs, args, project_id, output_version, logger)
    }

    df_map = df_choose.get(runtime)

    return df_map

def createClickhouTableSql(df, table_name, database='default', order_by='', partition_by='version'):
    def getSchemeSql(df):
        file_scheme = df.dtypes
        sql_scheme = ""
        for i in file_scheme:
            if i[0] in [order_by, partition_by]:
                # 主键和分区键不支持null
                coltype = i[1].capitalize()
            else:
                coltype = f"Nullable({i[1].capitalize()})"
            sql_scheme += f"`{i[0]}` {coltype},"
        sql_scheme = re.sub(r",$", "", sql_scheme)
        return sql_scheme
    return f"CREATE TABLE IF NOT EXISTS {database}.{table_name}({getSchemeSql(df)}) ENGINE = MergeTree() ORDER BY tuple({order_by}) PARTITION BY {partition_by};"


def http_create_table(sql_create_table):
    url = "https://max.pharbers.com/ch/"
    params = {
        "query": sql_create_table
    }

    res = requests.post(url=url, params=params)


def outClickhouse(df, table, mode="append", numPartitions = 2, database='default'):
    df.write.format("jdbc").mode(mode) \
        .option("url", "jdbc:clickhouse://192.168.16.117:8123/" + database) \
        .option("dbtable", table) \
        .option("driver", "ru.yandex.clickhouse.ClickHouseDriver") \
        .option("user", "default") \
        .option("password", "") \
        .option("batchsize", 1000) \
        .option("socket_timeout", 300000) \
        .option("numPartitions", numPartitions) \
        .option("rewrtieBatchedStatements", True) \
        .save()

def out_to_s3(df, ds_conf, logger):

    logger.debug("向s3写入数据")
    prop = ds_conf.get("prop")
    logger.debug(prop)
    logger.debug(type(prop))
    ds_format = prop.get("format")
    path = prop.get("path")
    partitions = prop.get("partitions")
    if ds_format == "Parquet":
        df.repartition(partitions).write.format("parquet") \
            .mode("append") \
            .save(path)
    elif ds_format == "CSV":
        df.repartition(partitions).write.format("csv")\
            .option("header", "true") \
            .mode("overwrite")\
            .save(path)

def addVersion(df, version, version_colname='version'):
    df = df.withColumn(version_colname, lit(version))
    return df


def deleteTableVersionSql(table, content, colname='version', database='default'):
    sql_content = f"ALTER TABLE {database}.{table} DELETE WHERE `{colname}`=='{content}'"
    return sql_content


def put_scheme_to_dataset(outputs_id, project_id, schema_list, output_version, logger):

    dynamodb_resource = boto3.resource("dynamodb", region_name="cn-northwest-1")
    table = dynamodb_resource.Table("dataset")
    key = {
        "id": outputs_id,
        "projectId": project_id
    }
    res = table.get_item(
        Key=key,
    )
    dataset_item = res.get("Item")
    dataset_item.update({"schema": json.dumps(schema_list, ensure_ascii=False)})
    dataset_item.update({"version": json.dumps(output_version, ensure_ascii=False)})

    table.put_item(
        Item=dataset_item
    )


def putOutputSchema(outputs_id, project_id, df, output_version, logger):

    def getSchemaSql(df):
        schema_list = []
        file_scheme = df.dtypes
        for i in file_scheme:
            schema_map = {}
            schema_map["src"] = i[0]
            schema_map["des"] = i[0]
            schema_map["type"] = i[1].capitalize().replace('Int', 'Double')
            schema_list.append(schema_map)
        return schema_list

    schema_list = getSchemaSql(df)
    logger.debug("json转换schemaList")
    logger.debug(schema_list)
    logger.debug(type(schema_list))
    logger.debug(json.dumps(schema_list, ensure_ascii=False))
    res = put_scheme_to_dataset(outputs_id[0], project_id, schema_list, output_version, logger)

def changeIntToDouble(df):
    for colname,coltype in df.dtypes:
        if coltype == 'int':
            df = df.withColumn(colname, df[colname].cast('double'))
    return df


def createOutputs(args, ph_conf, outputs, outputs_id, project_id, inputs, output_version, logger):

    out_df = args.get("out_df")
    if out_df:

        out_df = changeIntToDouble(out_df)
        table_name = project_id + "_" + outputs[0]

        logger.debug(table_name)
        logger.debug(output_version)
        logger.debug(table_name)

        add_version_df = addVersion(out_df, output_version, version_colname='version')
        logger.debug("df添加version完成")
        sql_create_table = createClickhouTableSql(add_version_df, table_name)
        logger.debug(sql_create_table)
        logger.debug("根据df创建建表语句")
        http_create_table(sql_create_table)
        logger.debug("根据建表语句创建表成功")

        putOutputSchema(outputs_id, project_id, add_version_df, output_version, logger)
        logger.debug("向dynamodb写入dataset_scheme成功")

        sql_delete_version = deleteTableVersionSql(table_name, output_version)
        logger.debug("创建删除语句")
        http_create_table(sql_delete_version)
        logger.debug("根据删除语句删除成功")

        ds_list = args.get("ds_conf")
        output_name = outputs[0]
        name_ds_map_dict = {}
        for ds in ds_list:
            name_ds_map_dict.update({ds.get("name"): ds})
        if output_name in name_ds_map_dict.keys():
            logger.debug(name_ds_map_dict.get(output_name))
            logger.debug(type(name_ds_map_dict.get(output_name)))
            out_to_s3(add_version_df, name_ds_map_dict.get(output_name), logger)
        else:
            # 获取
            outClickhouse(add_version_df, table_name)
            logger.debug("向创建的表写入数据成功")


$alfred_debug_execute


if __name__ == '__main__':
    debug_execute()


