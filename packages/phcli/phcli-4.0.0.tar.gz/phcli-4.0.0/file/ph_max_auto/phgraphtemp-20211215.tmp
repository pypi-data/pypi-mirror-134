import os
import uuid
import time
import json
import boto3
import math
import random
import string
import datetime
import subprocess
from datetime import timedelta
from airflow.utils.dates import days_ago
from airflow.models import DAG, Variable
from airflow.operators.bash_operator import BashOperator
from airflow.operators.python_operator import PythonOperator
from airflow.utils.trigger_rule import TriggerRule

emr_client = boto3.client('emr')
s3_client = boto3.client('s3')
dynamodb_resource = boto3.resource("dynamodb")
ACTIONONFAILURE = "CONTINUE"
STEP_JAR = "command-runner.jar"

default_args = {
    "owner": "$alfred_dag_owner",
    "start_date": days_ago($alfred_start_date),
    "email": $alfred_email,
    "email_on_failure": $alfred_email_on_failure,
    "email_on_retry": $alfred_email_on_retry,
    "retries": $alfred_retries,
    "retry_delay": timedelta($alfred_retry_delay),
    "projectId": "$alfred_projectId"
}

dag = DAG(
    dag_id="$alfred_dag_id",
    tags=[$alfred_dag_tags],
    default_args=default_args,
    schedule_interval=$alfred_schedule_interval,
    description="$alfred_description",
    dagrun_timeout=timedelta(minutes=$alfred_dag_timeout)
)

default_status = {
    "start": "running",
    "end": "success",
    "error": "failed",
}

var_key_lst = Variable.get("%s__SPARK_CONF" % (dag.dag_id), deserialize_json=True, default_var={})

# subprocess Ponen CMD
def process_cmd(cmd):
    print("process: " + cmd)

    p = subprocess.Popen(cmd, shell=True, text=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
    last_line = ''
    while p.poll() is None:
        line = p.stdout.read().strip("\n")
        if line:
            last_line = line
            print(last_line)
    if p.returncode == 0:
        print('Subprogram success')
    else:
        raise Exception(last_line)

def create_step_args(run_id, dag_name, job_full_name, owner, parameters):

    args = ["spark-submit",
            "--deploy-mode", "cluster",
            "--conf", "spark.driver.cores=1",
            "--conf", "spark.driver.memory=1g",
            "--conf", "spark.executor.cores=1",
            "--conf", "spark.executor.memory=1g",
            "--conf", "spark.executor.instances=1",
            "--conf", "spark.executor.extraJavaOptions=-Dfile.encoding=UTF-8 -Dsun.jnu.encoding=UTF-8",
            "--conf", "spark.driver.extraJavaOptions=-Dfile.encoding=UTF-8 -Dsun.jnu.encoding=UTF-8",
            "--jars", "s3://ph-platform/2020-11-11/emr/client/clickhouse-connector/clickhouse-jdbc-0.2.4.jar,s3://ph-platform/2020-11-11/emr/client/clickhouse-connector/guava-30.1.1-jre.jar",
            "--py-files",
            "s3://ph-platform/2020-11-11/jobs/python/phcli/common/phcli-4.0.0-py3.8.egg,s3://ph-platform/2020-11-11/jobs/python/phcli/"+dag_name+"/"+job_full_name+"/phjob.py",
            "s3://ph-platform/2020-11-11/jobs/python/phcli/"+dag_name+"/"+job_full_name+"/phmain.py",
            "--owner", owner,
            "--dag_name", dag_name,
            "--run_id", run_id,
            "--job_full_name", job_full_name,
            "--ph_conf", json.dumps(parameters, ensure_ascii=False).replace("}}", "} }").replace("{{", "{ {"),
            "--job_id", "not_implementation"
            ]

    return args

def get_cluster_id():
    res = emr_client.list_clusters(
        ClusterStates=[
            "WAITING",
            "RUNNING"
        ]
    )
    cluster_ids = list([cluster['Id'] for cluster in res.get('Clusters') if cluster['Name'] =="phdev"])
    return cluster_ids[0]

def run_emr_step(dag_name, job_full_name, args_list=None):
    cluster_id = get_cluster_id()
    step_name = dag_name + "_" + job_full_name

    step = {}
    step.update({"Name": step_name})
    step.update({"ActionOnFailure": ACTIONONFAILURE})
    step.update({"HadoopJarStep": {}})
    step["HadoopJarStep"].update({"Jar": STEP_JAR})
    step["HadoopJarStep"].update({"Args": args_list})
    steps=[]
    steps.append(step)
    run_step_response = emr_client.add_job_flow_steps(
        JobFlowId=cluster_id,
        Steps=steps
    )

    while run_step_response:
        time.sleep(30)
        step_information_response = emr_client.describe_step(
            ClusterId=cluster_id,
            StepId=run_step_response["StepIds"][0]
        )
        step_statuses = ["COMPLETED", "FAILED", "CANCELLED", "INTERRUPTED"]

        if step_information_response['Step']['Status']['State'] in step_statuses:
            step_id = run_step_response["StepIds"][0]
            step_status = step_information_response['Step']['Status']['State']
            break

    emr_log = "s3://ph-platform/2020-11-11/emr/logs/" + cluster_id + "/steps/" + step_id + "/"

    return {
        "emr_log": emr_log,
        "step_status": step_status
    }

def put_item(run_id, job_id, airflowRunId, logType, localLog, step_message, lmdLog=None ,sfnLog=None):
    data={}
    data.update({"runId": run_id})
    data.update({"jobId": job_id})
    data.update({"airflowRunId": airflowRunId})
    data.update({"logType": logType})
    data.update({"localLog": localLog})
    data.update({"emrLog": step_message.get("emr_log")})
    data.update({"lmdLog": lmdLog})
    data.update({"sfnLog": sfnLog})

    table_name = "logs"
    table = dynamodb_resource.Table(table_name)
    table.put_item(
        Item=data
    )

    if step_message.get("step_status") == "FAILED" or step_message.get("step_status") == "CANCELLED":
        raise Exception ("job 运行出错")
    return {
        "data": data
    }

def generate():
    charset = "ABCDEFGHIJKLMNOPQRSTUVWXYZ" \
              "abcdefghijklmnopqrstuvwxyz" \
              "0123456789-_"

    charsetLength = len(charset)
    keyLength = 3 * 5

    array = []
    for i in range(keyLength):
        array.append(charset[math.floor(random.random() * charsetLength)])

    return "".join(array)

def sync_notification(id, status, job_full_name, run_id, job_show_name):
    data = {}
    data.update({"id": run_id})
    data.update({"projectId": job_full_name})
    data.update({"code": "0"})
    data.update({"comments": "<empty>"})
    data.update({"date": str(round(time.time() * 1000))})
    data.update({"jobDesc": "max"})
    message = {
          "type": "notification",
          "opname": default_args["owner"],
          "cnotification": {
              "jobName": job_full_name,
              "runId" : run_id,
              "jobShowName" : job_show_name,
              "error": ""
          }
      }
    data.update({"message": json.dumps(message, ensure_ascii=False)})
    data.update({"owner": ""})
    data.update({"showName": default_args["owner"]})
    data.update({"jobCat": status})

    table_name = "notification"
    table = dynamodb_resource.Table(table_name)
    table.put_item(
        Item=data
    )