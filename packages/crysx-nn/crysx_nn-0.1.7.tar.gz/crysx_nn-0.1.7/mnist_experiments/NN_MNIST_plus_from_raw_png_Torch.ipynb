{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bb703b4",
   "metadata": {},
   "source": [
    "# This is a demo of how to load MNIST raw pngs manually without torchvision and use torch for neural network machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f72d31",
   "metadata": {},
   "source": [
    "## Run the following for Google colab \n",
    "then restart runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb15288",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade --no-cache-dir https://github.com/manassharma07/crysx_nn/tarball/main\n",
    "! pip install IPython==7.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d3fab4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from crysx_nn import mnist_utils as mu\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec255f0",
   "metadata": {},
   "source": [
    "## Download MNIST_orig and MNIST_plus dataset (May take upto 5 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ffade76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mu.downloadMNIST()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eff2a4e",
   "metadata": {},
   "source": [
    "## Load the training dataset from MNIST_plus in memory (May take upto 5 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faa534ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "path = 'MNIST-PLUS-PNG/mnist_plus_png'\n",
    "trainData, trainLabels = mu.loadMNIST(path_main=path, train=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02b75697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape (62250, 28, 28)\n",
      "Training labels shape (62250, 1)\n",
      "Size of training data in memory (GB) 0.3636181354522705\n"
     ]
    }
   ],
   "source": [
    "print('Training data shape', trainData.shape)\n",
    "print('Training labels shape',trainLabels.shape)\n",
    "print('Size of training data in memory (GB)', trainData.nbytes/1024/1024/1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0065b3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "255.0\n",
      "32.67065896238013\n",
      "77.61966293557188\n"
     ]
    }
   ],
   "source": [
    "# Statistics\n",
    "print(trainData.min()) # Expected for MNIST_orig: 0.0\n",
    "print(trainData.max()) # Expected for MNIST_orig: 255.\n",
    "print(trainData.mean()) # Expected for MNIST_orig: 33.31842144\n",
    "print(trainData.std()) # Expected for MNIST_orig: 78.567489983"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a637000b",
   "metadata": {},
   "source": [
    "## Normalize within the range [0,1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9df9b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n",
      "0.1281202312250202\n",
      "0.304390835041458\n"
     ]
    }
   ],
   "source": [
    "trainData = trainData/255 # Normalize\n",
    "# Statistics\n",
    "print(trainData.min()) # Expected for MNIST_orig: 0.0\n",
    "print(trainData.max()) # Expected for MNIST_orig: 1.0\n",
    "print(trainData.mean()) # Expected for MNIST_orig: 0.1306604762738426\n",
    "print(trainData.std()) # Expected for MNIST_orig: 0.3081078038564622 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902ed0fd",
   "metadata": {},
   "source": [
    "## Standardize the data so that it has mean 0 and variance 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "015a9fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.4209069934959449\n",
      "2.864343036662815\n",
      "-2.0096869373509445e-16\n",
      "1.000000000000002\n"
     ]
    }
   ],
   "source": [
    "trainData = (trainData - np.mean(trainData)) / np.std(trainData)\n",
    "# Statistics\n",
    "print(trainData.min()) # Expected for MNIST_orig: -0.42407\n",
    "print(trainData.max()) # Expected for MNIST_orig: 2.8215433\n",
    "print(trainData.mean()) # Expected for MNIST_orig: 0.0\n",
    "print(trainData.std()) # Expected for MNIST_orig: 1.0000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da1b0af",
   "metadata": {},
   "source": [
    "## Convert labels to one-hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1796640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.]\n",
      " [6.]\n",
      " [8.]\n",
      " ...\n",
      " [7.]\n",
      " [2.]\n",
      " [7.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(trainLabels)\n",
    "trainLabels = mu.one_hot_encode(trainLabels, 10)\n",
    "print(trainLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2c013c",
   "metadata": {},
   "source": [
    "## Convert numpy arrays to torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70109dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "trainData_torch = torch.Tensor(trainData).float()\n",
    "trainLabels_torch = torch.Tensor(trainLabels).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f82965",
   "metadata": {},
   "source": [
    "## Let us train a NN now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "756390d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Flatten(start_dim=1, end_dim=-1)\n",
      "  (1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (4): Softmax(dim=1)\n",
      ")\n",
      "203530 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "### Choose device: 'cuda' or 'cpu'\n",
    "device = 'cpu'\n",
    "# device = 'cuda'\n",
    "\n",
    "### Define the dense neuron layer\n",
    "# Network = torch.nn.Sequential(\n",
    "#     torch.nn.Flatten(),            # 28x28 -> 784\n",
    "#     torch.nn.Linear(784, 10),      # 784 -> 10\n",
    "#     torch.nn.Softmax(dim=1)\n",
    "# )\n",
    "Network = torch.nn.Sequential(\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(784, 256),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(256, 10),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")\n",
    "Network.to(device=device)\n",
    "\n",
    "### Get information about model\n",
    "totpars = 0\n",
    "for par in Network.parameters():\n",
    "    newpars = 1\n",
    "    for num in par.shape:\n",
    "        newpars *= num\n",
    "    totpars += newpars\n",
    "print(Network)\n",
    "print('%i trainable parameters' % totpars)\n",
    "\n",
    "### Initialize loss function and optimizer\n",
    "crit = torch.nn.BCELoss()\n",
    "# crit = torch.nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(Network.parameters(), lr=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f7041f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3251)\n"
     ]
    }
   ],
   "source": [
    "### Baseline: just say it's anything at probability 1/N, what's the loss?\n",
    "N = 10\n",
    "labels = torch.zeros(1, 10, dtype=torch.float32)\n",
    "labels[0, 3] = 1.\n",
    "output = torch.full_like(labels, 1./N)\n",
    "print(crit(output, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b167d496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311\n"
     ]
    }
   ],
   "source": [
    "batchSize = 200\n",
    "nBatches = trainData_torch.shape[0]//batchSize\n",
    "print(nBatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c11c303f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffe424ea011e44e7a7af7a116a8d5a3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.08290213516574964\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.045554701131064794\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.035205853234533326\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 0.028517881165770663\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 0.02390750354736852\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 0.020525507167748317\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 0.017943621776291316\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 0.015907989278842974\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 0.014254891108481927\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0.01287994366378742\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "### Set model in training mode and create the epochs axis\n",
    "Network.train()\n",
    "epochs = range(1, 11)\n",
    "\n",
    "errorPlot = []\n",
    "\n",
    "### Train the model\n",
    "for e in tqdm(epochs):\n",
    "    tr_loss = 0.\n",
    "    samples = 0\n",
    "    ### Loop over batches\n",
    "    for iBatch in tqdm(range(nBatches),leave=False):\n",
    "        offset = iBatch*batchSize\n",
    "        inputsBatch = trainData_torch[offset:offset + batchSize,:,:]# Input vector\n",
    "#            print(x.shape)\n",
    "        labelsBatch = trainLabels_torch[offset:offset + batchSize,:] # Expected output\n",
    "        \n",
    "        opt.zero_grad() # zero gradient values\n",
    "        inputsBatch = inputsBatch.to(device=device) # move input and label tensors to the device with the model\n",
    "        labelsBatch = labelsBatch.to(device=device)\n",
    "        outputsTorch = Network(inputsBatch) # compute model outputs\n",
    "        loss = crit(outputsTorch, labelsBatch) # compute batch loss\n",
    "        loss.backward() # back-propagate the gradients\n",
    "        opt.step() # update the model weights\n",
    "        tr_loss += loss.clone().cpu().item()*len(inputsBatch) # add the batch loss to the running loss\n",
    "        samples += len(inputsBatch) # update the number of processed samples\n",
    "    tr_loss /= samples # compute training loss\n",
    "    errorPlot.append(tr_loss)\n",
    "    print(e, tr_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443ff259",
   "metadata": {},
   "source": [
    "## Test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9261ebc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data shape (10000, 28, 28)\n",
      "Test labels shape (10000, 1)\n",
      "Size of training data in memory (GB) 0.05841255187988281\n",
      "0.0\n",
      "255.0\n",
      "33.791224489795916\n",
      "79.17246322228647\n",
      "0.0\n",
      "1.0\n",
      "0.13251460584233696\n",
      "0.31048024793053497\n",
      "2.0096869373509406e-16\n",
      "0.9999999999999982\n",
      "0.13251460584233696\n",
      "0.3104802479305344\n",
      "[[5.]\n",
      " [3.]\n",
      " [4.]\n",
      " ...\n",
      " [4.]\n",
      " [8.]\n",
      " [7.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "path = 'MNIST-PLUS-PNG/mnist_plus_png'\n",
    "testData, testLabels = mu.loadMNIST(path_main=path, train=False, shuffle=True)\n",
    "\n",
    "print('Test data shape', testData.shape)\n",
    "print('Test labels shape',testLabels.shape)\n",
    "print('Size of training data in memory (GB)', testData.nbytes/1024/1024/1024)\n",
    "\n",
    "# Statistics\n",
    "print(testData.min()) # Expected for MNIST_orig: 0.0\n",
    "print(testData.max()) # Expected for MNIST_orig: 255.\n",
    "print(testData.mean()) # Expected for MNIST_orig: 33.31842144\n",
    "print(testData.std()) # Expected for MNIST_orig: 78.567489983\n",
    "\n",
    "## Normalize within the range [0,1.0]\n",
    "\n",
    "testData = testData/255 # Normalize\n",
    "# Statistics\n",
    "print(testData.min()) # Expected for MNIST_orig: 0.0\n",
    "print(testData.max()) # Expected for MNIST_orig: 1.0\n",
    "print(testData.mean()) # Expected for MNIST_orig: 0.1306604762738426\n",
    "print(testData.std()) # Expected for MNIST_orig: 0.3081078038564622 \n",
    "\n",
    "## Standardize the data so that it has mean 0 and variance 1\n",
    "# Use the mean and std of training data **********\n",
    "testData = (testData - np.mean(trainData)) / np.std(trainData)\n",
    "# Statistics\n",
    "print(testData.min()) # Expected for MNIST_orig: -0.42407\n",
    "print(testData.max()) # Expected for MNIST_orig: 2.8215433\n",
    "print(testData.mean()) # Expected for MNIST_orig: 0.0\n",
    "print(testData.std()) # Expected for MNIST_orig: 1.0000\n",
    "\n",
    "## Convert labels to one-hot vectors\n",
    "print(testLabels)\n",
    "testLabels = mu.one_hot_encode(testLabels, 10)\n",
    "print(testLabels)\n",
    "\n",
    "## Convert numpy arrays to torch tensors\n",
    "testData_torch = torch.Tensor(testData).float()\n",
    "testLabels_torch = torch.Tensor(testLabels).float()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72abe86",
   "metadata": {},
   "source": [
    "## Performance on Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00d9c432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "batchSize = 200\n",
    "nBatches = testData_torch.shape[0]//batchSize\n",
    "print(nBatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29e291aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.105020, accuracy: 0.961500\n"
     ]
    }
   ],
   "source": [
    "### Set model in evaluation mode\n",
    "Network.eval()\n",
    "\n",
    "### Compute the test loss\n",
    "with torch.no_grad():\n",
    "    te_loss = 0.\n",
    "    samples = 0\n",
    "    accuracy = 0\n",
    "     ### Loop over batches\n",
    "    for iBatch in tqdm(range(nBatches),leave=False):\n",
    "        offset = iBatch*batchSize\n",
    "        inputs = testData_torch[offset:offset + batchSize,:,:]# Input vector\n",
    "#            print(x.shape)\n",
    "        labels = testLabels_torch[offset:offset + batchSize,:] # Expected output\n",
    "        inputs = inputs.to(device=device)\n",
    "        labels = labels.to(device=device)\n",
    "        outputs = Network(inputs)\n",
    "        loss = crit(outputs, labels)\n",
    "        te_loss += loss.clone().cpu().item()*len(inputs)\n",
    "        accuracy += torch.sum(torch.eq(torch.max(labels, 1)[1], torch.max(outputs, 1)[1]), dtype=int).clone().cpu().item()\n",
    "        samples += len(inputs)\n",
    "    te_loss /= samples\n",
    "    accuracy /= samples\n",
    "    print('Test loss: %f, accuracy: %f' % (te_loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfb79a7",
   "metadata": {},
   "source": [
    "## Interactive test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "974f49b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4013e-45,\n",
      "         1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]])\n",
      "6\n",
      "tensor([1.0000e+00, 1.4013e-45, 0.0000e+00])\n",
      "tensor([6, 5, 0])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import ImageTk, Image, ImageDraw\n",
    "import PIL\n",
    "from tkinter import *\n",
    "import cv2\n",
    "import torchvision\n",
    "\n",
    "width = 200  # canvas width\n",
    "height = 200 # canvas height\n",
    "center = height//2\n",
    "white = (255, 255, 255) # canvas back\n",
    "\n",
    "def save():\n",
    "    # save image to hard drive\n",
    "    filename = \"user_input.jpg\"\n",
    "    global output_image \n",
    "    output_image.save(filename)\n",
    "    ###### Centering begin\n",
    "    # Load image as grayscale and obtain bounding box coordinates\n",
    "    image = cv2.imread('user_input.jpg', 0)\n",
    "#     print(image)\n",
    "    height, width = image.shape\n",
    "    x,y,w,h = cv2.boundingRect(image)\n",
    "\n",
    "    # Create new blank image and shift ROI to new coordinates\n",
    "    ROI = image[y:y+h, x:x+w]\n",
    "    mask = np.zeros([ROI.shape[0]+10,ROI.shape[1]+10])\n",
    "    width, height = mask.shape\n",
    "#     print(ROI.shape)\n",
    "#     print(mask.shape)\n",
    "    x = width//2 - ROI.shape[0]//2 \n",
    "    y = height//2 - ROI.shape[1]//2 \n",
    "#     print(x,y)\n",
    "    mask[y:y+h, x:x+w] = ROI\n",
    "#     print(mask)\n",
    "    # Check if centering/masking was successful\n",
    "#     plt.imshow(mask, cmap='viridis') \n",
    "    output_image = PIL.Image.fromarray(mask)\n",
    "    compressed_output_image = output_image.resize((22,22))\n",
    "#     # Enhance Saturation\n",
    "#     converter = PIL.ImageEnhance.Color(compressed_output_image)\n",
    "#     compressed_output_image = converter.enhance(2.5)\n",
    "    # Enhance contrast\n",
    "#     converter = PIL.ImageEnhance.Contrast(compressed_output_image)\n",
    "#     compressed_output_image = converter.enhance(3.5)\n",
    "    \n",
    "    convert_tensor = torchvision.transforms.ToTensor()\n",
    "    tensor_image = convert_tensor(compressed_output_image)\n",
    "    tensor_image = torch.nn.functional.pad(tensor_image, (3,3,3,3), \"constant\", 0)\n",
    "    # Normalization shoudl be done after padding i guess\n",
    "    convert_tensor = torchvision.transforms.Normalize((0.1307), (0.3081))\n",
    "    tensor_image = convert_tensor(tensor_image)\n",
    "    plt.imshow(tensor_image.detach().cpu().numpy().reshape(28,28), cmap='viridis')\n",
    "    # Debugging\n",
    "#     print(tensor_image)\n",
    "#     print(np.array(compressed_output_image.getdata())) # Get data values)\n",
    "#     print(np.array(image.getdata()))\n",
    "\n",
    "    ### Compute the predictions\n",
    "    with torch.no_grad():\n",
    "        output0 = Network(torch.unsqueeze(tensor_image, dim=0).to(device=device))\n",
    "        print(output0)\n",
    "        certainty, output = torch.max(output0[0], 0)\n",
    "        certainty = certainty.clone().cpu().item()\n",
    "        output = output.clone().cpu().item()\n",
    "        certainty1, output1 = torch.topk(output0[0],3)\n",
    "        certainty1 = certainty1.clone().cpu()#.item()\n",
    "        output1 = output1.clone().cpu()#.item()\n",
    "#     print(certainty)\n",
    "    print(output)\n",
    "        \n",
    "    print(certainty1)\n",
    "    print(output1)\n",
    "\n",
    "def paint(event):\n",
    "    x1, y1 = (event.x - 1), (event.y - 1)\n",
    "    x2, y2 = (event.x + 1), (event.y + 1)\n",
    "#     canvas.create_oval(x1, y1, x2, y2, fill=\"white\",width=24)\n",
    "    canvas.create_rectangle(x1, y1, x2, y2, fill=\"white\",width=12)\n",
    "    draw.line([x1, y1, x2, y2],fill=\"white\",width=4)\n",
    "\n",
    "master = Tk()\n",
    "\n",
    "# create a tkinter canvas to draw on\n",
    "canvas = Canvas(master, width=width, height=height, bg='white')\n",
    "canvas.pack()\n",
    "\n",
    "# create an empty PIL image and draw object to draw on\n",
    "output_image = PIL.Image.new(\"L\", (width, height), 0)\n",
    "draw = ImageDraw.Draw(output_image)\n",
    "canvas.pack(expand=YES, fill=BOTH)\n",
    "canvas.bind(\"<B1-Motion>\", paint)\n",
    "\n",
    "# add a button to save the image\n",
    "button=Button(text=\"save\",command=save)\n",
    "button.pack()\n",
    "\n",
    "master.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd01cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
