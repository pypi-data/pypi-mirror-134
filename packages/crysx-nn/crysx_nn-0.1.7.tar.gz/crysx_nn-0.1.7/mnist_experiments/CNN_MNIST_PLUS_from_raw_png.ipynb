{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bb703b4",
   "metadata": {},
   "source": [
    "# This is a demo of how to load MNIST_plus raw pngs manually without torchvision and use torch for neural network machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97660a4",
   "metadata": {},
   "source": [
    "## Run the following for Google colab \n",
    "then restart runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37e8c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade --no-cache-dir https://github.com/manassharma07/crysx_nn/tarball/main\n",
    "! pip install IPython==7.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d3fab4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from crysx_nn import mnist_utils as mu\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51846e1d",
   "metadata": {},
   "source": [
    "## Download MNIST_orig and MNIST_plus dataset (May take upto 5 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b279c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mu.downloadMNIST()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d865ec5",
   "metadata": {},
   "source": [
    "## Load the training dataset from MNIST_plus in memory (May take upto 5 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ffade76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "path = 'MNIST-PLUS-PNG/mnist_plus_png'\n",
    "trainData, trainLabels = mu.loadMNIST(path_main=path, train=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02b75697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape (62250, 28, 28)\n",
      "Training labels shape (62250, 1)\n",
      "Size of training data in memory (GB) 0.3636181354522705\n"
     ]
    }
   ],
   "source": [
    "print('Training data shape', trainData.shape)\n",
    "print('Training labels shape',trainLabels.shape)\n",
    "print('Size of training data in memory (GB)', trainData.nbytes/1024/1024/1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0065b3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "255.0\n",
      "32.67065896238013\n",
      "77.6196629355719\n"
     ]
    }
   ],
   "source": [
    "# Statistics\n",
    "print(trainData.min()) # Expected for MNIST_orig: 0.0\n",
    "print(trainData.max()) # Expected for MNIST_orig: 255.\n",
    "print(trainData.mean()) # Expected for MNIST_orig: 33.31842144\n",
    "print(trainData.std()) # Expected for MNIST_orig: 78.567489983"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a637000b",
   "metadata": {},
   "source": [
    "## Normalize within the range [0,1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9df9b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n",
      "0.12812023122501984\n",
      "0.30439083504145775\n"
     ]
    }
   ],
   "source": [
    "trainData = trainData/255 # Normalize\n",
    "# Statistics\n",
    "print(trainData.min()) # Expected for MNIST_orig: 0.0\n",
    "print(trainData.max()) # Expected for MNIST_orig: 1.0\n",
    "print(trainData.mean()) # Expected for MNIST_orig: 0.1306604762738426\n",
    "print(trainData.std()) # Expected for MNIST_orig: 0.3081078038564622 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902ed0fd",
   "metadata": {},
   "source": [
    "## Standardize the data so that it has mean 0 and variance 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "015a9fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.42090699349594407\n",
      "2.8643430366628184\n",
      "9.970753106947298e-16\n",
      "1.0000000000000004\n"
     ]
    }
   ],
   "source": [
    "trainData = (trainData - np.mean(trainData)) / np.std(trainData)\n",
    "# Statistics\n",
    "print(trainData.min()) # Expected for MNIST_orig: -0.42407\n",
    "print(trainData.max()) # Expected for MNIST_orig: 2.8215433\n",
    "print(trainData.mean()) # Expected for MNIST_orig: 0.0\n",
    "print(trainData.std()) # Expected for MNIST_orig: 1.0000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da1b0af",
   "metadata": {},
   "source": [
    "## Convert labels to one-hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbb66b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(a, num_classes):\n",
    "    return np.squeeze(np.eye(num_classes)[a.astype(int).reshape(-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1796640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.]\n",
      " [3.]\n",
      " [2.]\n",
      " ...\n",
      " [5.]\n",
      " [1.]\n",
      " [7.]]\n",
      "[[0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(trainLabels)\n",
    "trainLabels = one_hot_encode(trainLabels, 10)\n",
    "print(trainLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2c013c",
   "metadata": {},
   "source": [
    "## Convert numpy arrays to torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70109dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "trainData_torch = torch.Tensor(trainData).float()\n",
    "trainLabels_torch = torch.Tensor(trainLabels).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3bf7bb",
   "metadata": {},
   "source": [
    "## For Convolutional Layers in Torch we need to add one more dimension\n",
    "https://stackoverflow.com/questions/57237381/runtimeerror-expected-4-dimensional-input-for-4-dimensional-weight-32-3-3-but"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebf9ef7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([62250, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "trainData_torch = trainData_torch.unsqueeze(1)  # if torch tensor\n",
    "print(trainData_torch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f82965",
   "metadata": {},
   "source": [
    "## Let us train a CNN now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "756390d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 12, kernel_size=(9, 9), stride=(1, 1))\n",
      "  (1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (2): ReLU()\n",
      "  (3): Conv2d(12, 24, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (4): ReLU()\n",
      "  (5): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (6): Flatten(start_dim=1, end_dim=-1)\n",
      "  (7): Linear(in_features=96, out_features=256, bias=True)\n",
      "  (8): ReLU()\n",
      "  (9): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (10): Softmax(dim=1)\n",
      ")\n",
      "35610 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "### Choose device: 'cuda' or 'cpu'\n",
    "device = 'cpu'\n",
    "# device = 'cuda'\n",
    "\n",
    "Network = torch.nn.Sequential(      #  1x28x28\n",
    "    torch.nn.Conv2d(1, 12, (9, 9)),  #  12x20x20\n",
    "    torch.nn.MaxPool2d((2, 2)),     #  12x10x10\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Conv2d(12, 24, (5, 5)), # 24x 6x 6\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d((3, 3)),     # 24x 2x 2\n",
    "    torch.nn.Flatten(),             #       96\n",
    "    torch.nn.Linear(96, 256),        #       16\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(256, 10),        #       10\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")\n",
    "Network.to(device=device)\n",
    "\n",
    "### Get information about model\n",
    "totpars = 0\n",
    "for par in Network.parameters():\n",
    "    newpars = 1\n",
    "    for num in par.shape:\n",
    "        newpars *= num\n",
    "    totpars += newpars\n",
    "print(Network)\n",
    "print('%i trainable parameters' % totpars)\n",
    "\n",
    "### Initialize loss function and optimizer\n",
    "crit = torch.nn.BCELoss()\n",
    "# crit = torch.nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(Network.parameters(), lr=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f7041f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3251)\n"
     ]
    }
   ],
   "source": [
    "### Baseline: just say it's anything at probability 1/N, what's the loss?\n",
    "N = 10\n",
    "labels = torch.zeros(1, 10, dtype=torch.float32)\n",
    "labels[0, 3] = 1.\n",
    "output = torch.full_like(labels, 1./N)\n",
    "print(crit(output, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b167d496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311\n"
     ]
    }
   ],
   "source": [
    "batchSize = 200\n",
    "nBatches = trainData_torch.shape[0]//batchSize\n",
    "print(nBatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c11c303f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a31766aeba034ec39195f76ee5184523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.10242332758867088\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.02351741150614485\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.016246456543576582\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 0.01293475177944593\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 0.010930683943697542\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 0.009484995050966692\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 0.008404665336857318\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 0.007526769116371151\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 0.006796771298474078\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0.006181066734132348\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 0.005620742455713295\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 0.00515503910204719\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 0.004743866523625958\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 0.0043635524603855644\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 0.004027480576498063\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 0.0036973487716797033\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 0.0034034081980461777\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "### Set model in training mode and create the epochs axis\n",
    "Network.train()\n",
    "epochs = range(1, 18)\n",
    "\n",
    "errorPlot = []\n",
    "\n",
    "### Train the model\n",
    "for e in tqdm(epochs):\n",
    "    tr_loss = 0.\n",
    "    samples = 0\n",
    "    ### Loop over batches\n",
    "    for iBatch in tqdm(range(nBatches),leave=False):\n",
    "        offset = iBatch*batchSize\n",
    "        inputsBatch = trainData_torch[offset:offset + batchSize,:,:,:]# Input vector\n",
    "#            print(x.shape)\n",
    "        labelsBatch = trainLabels_torch[offset:offset + batchSize,:] # Expected output\n",
    "        \n",
    "        opt.zero_grad() # zero gradient values\n",
    "        inputsBatch = inputsBatch.to(device=device) # move input and label tensors to the device with the model\n",
    "        labelsBatch = labelsBatch.to(device=device)\n",
    "        outputsTorch = Network(inputsBatch) # compute model outputs\n",
    "#         print(outputsTorch)\n",
    "        loss = crit(outputsTorch, labelsBatch) # compute batch loss\n",
    "        loss.backward() # back-propagate the gradients\n",
    "        opt.step() # update the model weights\n",
    "        tr_loss += loss.clone().cpu().item()*len(inputsBatch) # add the batch loss to the running loss\n",
    "        samples += len(inputsBatch) # update the number of processed samples\n",
    "    tr_loss /= samples # compute training loss\n",
    "    errorPlot.append(tr_loss)\n",
    "    print(e, tr_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443ff259",
   "metadata": {},
   "source": [
    "## Test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9261ebc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data shape (10000, 28, 28)\n",
      "Test labels shape (10000, 1)\n",
      "Size of training data in memory (GB) 0.05841255187988281\n",
      "0.0\n",
      "255.0\n",
      "33.791224489795916\n",
      "79.1724632222864\n",
      "0.0\n",
      "1.0\n",
      "0.13251460584233696\n",
      "0.31048024793053536\n",
      "9.970753106947298e-16\n",
      "-0.420966151823858\n",
      "2.8652645415708182\n",
      "0.014507413218327082\n",
      "1.020309720442113\n",
      "[[6.]\n",
      " [8.]\n",
      " [3.]\n",
      " ...\n",
      " [9.]\n",
      " [3.]\n",
      " [9.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "torch.Size([10000, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "path = 'MNIST-PLUS-PNG/mnist_plus_png'\n",
    "testData, testLabels = mu.loadMNIST(path_main=path, train=False, shuffle=True)\n",
    "\n",
    "print('Test data shape', testData.shape)\n",
    "print('Test labels shape',testLabels.shape)\n",
    "print('Size of training data in memory (GB)', testData.nbytes/1024/1024/1024)\n",
    "\n",
    "# Statistics\n",
    "print(testData.min()) # Expected for MNIST_orig: 0.0\n",
    "print(testData.max()) # Expected for MNIST_orig: 255.\n",
    "print(testData.mean()) # Expected for MNIST_orig: 33.31842144\n",
    "print(testData.std()) # Expected for MNIST_orig: 78.567489983\n",
    "\n",
    "## Normalize within the range [0,1.0]\n",
    "\n",
    "testData = testData/255 # Normalize\n",
    "# Statistics\n",
    "print(testData.min()) # Expected for MNIST_orig: 0.0\n",
    "print(testData.max()) # Expected for MNIST_orig: 1.0\n",
    "print(testData.mean()) # Expected for MNIST_orig: 0.1306604762738426\n",
    "print(testData.std()) # Expected for MNIST_orig: 0.3081078038564622 \n",
    "\n",
    "## Standardize the data so that it has mean 0 and variance 1\n",
    "# Use the mean and std of training data **********\n",
    "print(np.mean(trainData))\n",
    "testData = (testData - 0.1281) / 0.3043\n",
    "# Statistics\n",
    "print(testData.min()) # Expected for MNIST_orig: -0.42407\n",
    "print(testData.max()) # Expected for MNIST_orig: 2.8215433\n",
    "print(testData.mean()) # Expected for MNIST_orig: 0.0\n",
    "print(testData.std()) # Expected for MNIST_orig: 1.0000\n",
    "\n",
    "## Convert labels to one-hot vectors\n",
    "print(testLabels)\n",
    "testLabels = one_hot_encode(testLabels, 10)\n",
    "print(testLabels)\n",
    "\n",
    "## Convert numpy arrays to torch tensors\n",
    "testData_torch = torch.Tensor(testData).float()\n",
    "testLabels_torch = torch.Tensor(testLabels).float()\n",
    "\n",
    "## For Convolutional Layers in Torch we need to add one more dimension\n",
    "# https://stackoverflow.com/questions/57237381/runtimeerror-expected-4-dimensional-input-for-4-dimensional-weight-32-3-3-but\n",
    "\n",
    "testData_torch = testData_torch.unsqueeze(1)  # if torch tensor\n",
    "print(testData_torch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "218621ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(Network, 'model_torch_MNIST_plus_CNN_98_5.chk')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72abe86",
   "metadata": {},
   "source": [
    "## Performance on Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00d9c432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "batchSize = 200\n",
    "nBatches = testData_torch.shape[0]//batchSize\n",
    "print(nBatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "29e291aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.005502, accuracy: 0.990800\n"
     ]
    }
   ],
   "source": [
    "### Set model in evaluation mode\n",
    "Network.eval()\n",
    "\n",
    "### Compute the test loss\n",
    "with torch.no_grad():\n",
    "    te_loss = 0.\n",
    "    samples = 0\n",
    "    accuracy = 0\n",
    "     ### Loop over batches\n",
    "    for iBatch in tqdm(range(nBatches),leave=False):\n",
    "        offset = iBatch*batchSize\n",
    "        inputs = testData_torch[offset:offset + batchSize,:,:,:]# Input vector\n",
    "#            print(x.shape)\n",
    "        labels = testLabels_torch[offset:offset + batchSize,:] # Expected output\n",
    "        inputs = inputs.to(device=device)\n",
    "        labels = labels.to(device=device)\n",
    "        outputs = Network(inputs)\n",
    "        loss = crit(outputs, labels)\n",
    "        te_loss += loss.clone().cpu().item()*len(inputs)\n",
    "        accuracy += torch.sum(torch.eq(torch.max(labels, 1)[1], torch.max(outputs, 1)[1]), dtype=int).clone().cpu().item()\n",
    "        samples += len(inputs)\n",
    "    te_loss /= samples\n",
    "    accuracy /= samples\n",
    "    print('Test loss: %f, accuracy: %f' % (te_loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfb79a7",
   "metadata": {},
   "source": [
    "## Interactive test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "974f49b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(164, 105)\n",
      "(174, 115)\n",
      "5 5\n",
      "\n",
      "tensor([[2.1672e-06, 6.1167e-02, 8.3811e-04, 4.9362e-04, 5.2387e-05, 3.7252e-05,\n",
      "         1.9852e-06, 9.3605e-01, 1.6509e-04, 1.1906e-03]])\n",
      "7\n",
      "tensor([0.9361, 0.0612, 0.0012])\n",
      "tensor([7, 1, 9])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import ImageTk, Image, ImageDraw\n",
    "import PIL\n",
    "from tkinter import *\n",
    "import cv2\n",
    "import torchvision\n",
    "\n",
    "width = 200  # canvas width\n",
    "height = 200 # canvas height\n",
    "center = height//2\n",
    "white = (255, 255, 255) # canvas back\n",
    "\n",
    "def save():\n",
    "    # save image to hard drive\n",
    "    filename = \"user_input.png\"\n",
    "    global output_image \n",
    "    output_image.save(filename)\n",
    "    ###### Centering begin\n",
    "    # Load image as grayscale and obtain bounding box coordinates\n",
    "    image = cv2.imread('user_input.png', 0)\n",
    "    height, width = image.shape\n",
    "    x,y,w,h = cv2.boundingRect(image)\n",
    "\n",
    "\n",
    "    # Create new blank image and shift ROI to new coordinates\n",
    "    ROI = image[y:y+h, x:x+w]\n",
    "    mask = np.zeros([ROI.shape[0]+10,ROI.shape[1]+10])\n",
    "    width, height = mask.shape\n",
    "    print(ROI.shape)\n",
    "    print(mask.shape)\n",
    "    x = width//2 - ROI.shape[0]//2 \n",
    "    y = height//2 - ROI.shape[1]//2 \n",
    "    print(x,y)\n",
    "    mask[y:y+h, x:x+w] = ROI\n",
    "\n",
    "    # Check if centering/masking was successful\n",
    "#     plt.imshow(mask, cmap='viridis') \n",
    "    output_image = PIL.Image.fromarray(mask) # mask has values in [0-255] as expected\n",
    "    # Now we need to resize, but it causes problems with default arguments as it changes the range of pixel values to be negative or positive\n",
    "    # compressed_output_image = output_image.resize((22,22))\n",
    "    # Therefore, we use the following:\n",
    "    compressed_output_image = output_image.resize((22,22), PIL.Image.BILINEAR) # PIL.Image.NEAREST or PIL.Image.BILINEAR also performs good\n",
    "#     # Enhance Saturation\n",
    "#     converter = PIL.ImageEnhance.Color(compressed_output_image)\n",
    "#     compressed_output_image = converter.enhance(2.5)\n",
    "    # Enhance contrast\n",
    "#     converter = PIL.ImageEnhance.Contrast(compressed_output_image)\n",
    "#     compressed_output_image = converter.enhance(3.5)\n",
    "    convert_tensor = torchvision.transforms.ToTensor()\n",
    "    tensor_image = convert_tensor(compressed_output_image)\n",
    "    # Another problem we face is that in the above ToTensor() command, we should have gotten a normalized tensor with pixel values in [0,1]\n",
    "    # But somehow it doesn't happen. Therefore, we need to normalize manually\n",
    "    tensor_image = tensor_image/255.\n",
    "    # Padding\n",
    "    tensor_image = torch.nn.functional.pad(tensor_image, (3,3,3,3), \"constant\", 0)\n",
    "    # Normalization shoudl be done after padding i guess\n",
    "    convert_tensor = torchvision.transforms.Normalize((0.1281), (0.3043))\n",
    "    tensor_image = convert_tensor(tensor_image)\n",
    "    plt.imshow(tensor_image.detach().cpu().numpy().reshape(28,28), cmap='viridis')\n",
    "    # Debugging\n",
    "#     print(tensor_image)\n",
    "#     print(np.array(compressed_output_image.getdata())) # Get data values)\n",
    "#     print(np.array(image.getdata()))\n",
    "\n",
    "    ### Compute the predictions\n",
    "    print()\n",
    "    with torch.no_grad():\n",
    "#         print(tensor_image)\n",
    "        output0 = Network(torch.unsqueeze(tensor_image, dim=0).to(device=device))\n",
    "        print(output0)\n",
    "        certainty, output = torch.max(output0[0], 0)\n",
    "        certainty = certainty.clone().cpu().item()\n",
    "        output = output.clone().cpu().item()\n",
    "        certainty1, output1 = torch.topk(output0[0],3)\n",
    "        certainty1 = certainty1.clone().cpu()#.item()\n",
    "        output1 = output1.clone().cpu()#.item()\n",
    "#     print(certainty)\n",
    "    print(output)\n",
    "        \n",
    "    print(certainty1)\n",
    "    print(output1)\n",
    "\n",
    "def paint(event):\n",
    "    x1, y1 = (event.x - 1), (event.y - 1)\n",
    "    x2, y2 = (event.x + 1), (event.y + 1)\n",
    "#     canvas.create_oval(x1, y1, x2, y2, fill=\"white\",width=24)\n",
    "    canvas.create_rectangle(x1, y1, x2, y2, fill=\"white\",width=12)\n",
    "    draw.line([x1, y1, x2, y2],fill=\"white\",width=6)\n",
    "\n",
    "master = Tk()\n",
    "\n",
    "# create a tkinter canvas to draw on\n",
    "canvas = Canvas(master, width=width, height=height, bg='white')\n",
    "canvas.pack()\n",
    "\n",
    "# create an empty PIL image and draw object to draw on\n",
    "output_image = PIL.Image.new(\"L\", (width, height), 0)\n",
    "draw = ImageDraw.Draw(output_image)\n",
    "canvas.pack(expand=YES, fill=BOTH)\n",
    "canvas.bind(\"<B1-Motion>\", paint)\n",
    "\n",
    "# add a button to save the image\n",
    "button=Button(text=\"save\",command=save)\n",
    "button.pack()\n",
    "\n",
    "master.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa34680",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aeb79a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
