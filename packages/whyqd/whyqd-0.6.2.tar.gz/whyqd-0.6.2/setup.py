# -*- coding: utf-8 -*-
from setuptools import setup

packages = \
['whyqd',
 'whyqd.action',
 'whyqd.base',
 'whyqd.method',
 'whyqd.models',
 'whyqd.parsers',
 'whyqd.schema',
 'whyqd.types',
 'whyqd.validate']

package_data = \
{'': ['*']}

install_requires = \
['numpy>=1.21.1,<2.0.0',
 'openpyxl>=3.0.7,<4.0.0',
 'pandas>=1.3.1,<2.0.0',
 'pydantic>=1.8.2,<2.0.0',
 'tabulate>=0.8.9,<0.9.0',
 'xlrd>=2.0.1,<3.0.0']

setup_kwargs = {
    'name': 'whyqd',
    'version': '0.6.2',
    'description': 'data wrangling simplicity, complete audit transparency, and at speed',
    'long_description': '# whyqd: simplicity, transparency, speed\n\n[![Documentation Status](https://readthedocs.org/projects/whyqd/badge/?version=latest)](https://whyqd.readthedocs.io/en/latest/?badge=latest)\n[![Build Status](https://travis-ci.com/whythawk/whyqd.svg?branch=master)](https://travis-ci.com/whythawk/whyqd.svg?branch=master)\n[![DOI](https://zenodo.org/badge/239159569.svg)](https://zenodo.org/badge/latestdoi/239159569)\n\n## What is it?\n\n**whyqd** provides an intuitive method for restructuring messy data to conform to a standardised\nmetadata schema. It supports data managers and researchers looking to rapidly, and continuously,\nnormalise any messy spreadsheets using a simple series of steps. Once complete, you can import\nwrangled data into more complex analytical systems or full-feature wrangling tools.\n\nIt aims to get you to the point where you can perform automated data munging prior to\ncommitting your data into a database, and no further. It is built on Pandas, and plays well with\nexisting Python-based data-analytical tools. Each raw source file will produce a json schema and\nmethod file which defines the set of actions to be performed to produce refined data, and a\ndestination file validated against that schema.\n\n**whyqd** ensures complete audit transparency by saving all actions performed to restructure\nyour input data to a separate json-defined methods file. This permits others to read and scrutinise\nyour approach, validate your methodology, or even use your methods to import data in production.\n\nOnce complete, a method file can be shared, along with your input data, and anyone can\nimport **whyqd** and validate your method to verify that your output data is the product of these\ninputs.\n\n[Read the docs](https://whyqd.readthedocs.io/en/latest/) and there are two worked tutorials to demonstrate\nhow you can use `whyqd` to support source data curation transparency:\n\n- [Local-government data](https://whyqd.readthedocs.io/en/latest/tutorial_local_government_data.html)\n- [Data produced by Cthulhu](https://whyqd.readthedocs.io/en/latest/tutorial_cthulhu_data.html)\n\n## Why use it?\n\nIf all you want to do is test whether your source data are even useful, spending days or weeks\nslogging through data restructuring could kill a project. If you already have a workflow and\nestablished software which includes Python and pandas, having to change your code every time your\nsource data changes is really, really frustrating.\n\nIf you want to go from a Cthulhu dataset like this:\n\n![UNDP Human Development Index 2007-2008: a beautiful example of messy data.](https://raw.githubusercontent.com/whythawk/whyqd/master/docs/images/undp-hdi-2007-8.jpg)\n\nTo this:\n\n|     | country_name           | indicator_name | reference | year | values |\n| --: | :--------------------- | :------------- | :-------- | ---: | -----: |\n|   0 | Hong Kong, China (SAR) | HDI rank       | e         | 2008 |     21 |\n|   1 | Singapore              | HDI rank       | nan       | 2008 |     25 |\n|   2 | Korea (Republic of)    | HDI rank       | nan       | 2008 |     26 |\n|   3 | Cyprus                 | HDI rank       | nan       | 2008 |     28 |\n|   4 | Brunei Darussalam      | HDI rank       | nan       | 2008 |     30 |\n|   5 | Barbados               | HDI rank       | e,g, f    | 2008 |     31 |\n\nWith a readable set of scripts to ensure that your process can be audited and repeated:\n\n```\nscripts = [\n     "DEBLANK",\n     "DEDUPE",\n     "REBASE < [11]",\n     f"DELETE_ROWS < {[int(i) for i in np.arange(144, df.index[-1]+1)]}",\n     "RENAME_ALL > [\'HDI rank\', \'Country\', \'Human poverty index (HPI-1) - Rank;;2008\', \'Reference 1\', \'Human poverty index (HPI-1) - Value (%);;2008\', \'Probability at birth of not surviving to age 40 (% of cohort);;2000-05\', \'Reference 2\', \'Adult illiteracy rate (% aged 15 and older);;1995-2005\', \'Reference 3\', \'Population not using an improved water source (%);;2004\', \'Reference 4\', \'Children under weight for age (% under age 5);;1996-2005\', \'Reference 5\', \'Population below income poverty line (%) - $1 a day;;1990-2005\', \'Reference 6\', \'Population below income poverty line (%) - $2 a day;;1990-2005\', \'Reference 7\', \'Population below income poverty line (%) - National poverty line;;1990-2004\', \'Reference 8\', \'HPI-1 rank minus income poverty rank;;2008\']",\n     "PIVOT_CATEGORIES > [\'HDI rank\'] < [14,44,120]",\n     "RENAME_NEW > \'HDI Category\'::[\'PIVOT_CATEGORIES_idx_20_0\']",\n     "PIVOT_LONGER > = [\'HDI rank\', \'HDI Category\', \'Human poverty index (HPI-1) - Rank;;2008\', \'Human poverty index (HPI-1) - Value (%);;2008\', \'Probability at birth of not surviving to age 40 (% of cohort);;2000-05\', \'Adult illiteracy rate (% aged 15 and older);;1995-2005\', \'Population not using an improved water source (%);;2004\', \'Children under weight for age (% under age 5);;1996-2005\', \'Population below income poverty line (%) - $1 a day;;1990-2005\', \'Population below income poverty line (%) - $2 a day;;1990-2005\', \'Population below income poverty line (%) - National poverty line;;1990-2004\', \'HPI-1 rank minus income poverty rank;;2008\']",\n     "SPLIT > \';;\'::[\'PIVOT_LONGER_names_idx_9\']",\n     f"JOIN > \'reference\' < {reference_columns}",\n     "RENAME > \'indicator_name\' < [\'SPLIT_idx_11_0\']",\n     "RENAME > \'country_name\' < [\'Country\']",\n     "RENAME > \'year\' < [\'SPLIT_idx_12_1\']",\n     "RENAME > \'values\' < [\'PIVOT_LONGER_values_idx_10\']",\n  ]\n```\n\nThere are two complex and time-consuming parts to preparing data for analysis: social, and technical.\n\nThe social part requires multi-stakeholder engagement with source data-publishers, and with\ndestination database users, to agree structural metadata. Without any agreement on data publication\nformats or destination structure, you are left with the tedious frustration of manually wrangling\neach independent dataset into a single schema.\n\n**whyqd** allows you to get to work without requiring you to achieve buy-in from anyone or change\nyour existing code.\n\n## Wrangling process\n\n- Create, update or import a data schema which defines the destination data structure,\n- Create a new method and associate it with your schema and input data source/s,\n- Assign a foreign key column and (if required) merge input data sources,\n- Structure input data fields to conform to the requriements for each schema field,\n- Assign categorical data identified during structuring,\n- Transform and filter input data to produce a final destination data file,\n- Share your data and a citation.\n\n## Installation and dependencies\n\nYou\'ll need at least Python 3.7, then:\n\n`pip install whyqd`\n\nCode requirements have been tested on the following versions:\n\n- numpy>=1.18.1\n- openpyxl>=3.0.3\n- pandas>=1.0.0\n- tabulate>=0.8.3\n- xlrd>=1.2.0\n\nVersion 0.5.0 introduced a new, simplified, API, along with script-based transformation actions. You can import and\ntransform any saved `method.json` files with:\n\n```\nSCHEMA = whyqd.Schema(source=SCHEMA_SOURCE)\nschema_scripts = whyqd.parsers.LegacyScript().parse_legacy_method(\n            version="1", schema=SCHEMA, source_path=METHOD_SOURCE_V1\n        )\n```\n\nWhere SCHEMA_SOURCE is a path to your schema. Existing `schema.json` files should still work.\n\n## Changelog\n\nThe version history can be found in the [changelog](https://github.com/whythawk/whyqd/blob/master/CHANGELOG).\n\n## Background\n\n**whyqd** was created to serve a continuous data wrangling process, including collaboration on more\ncomplex messy sources, ensuring the integrity of the source data, and producing a complete audit\ntrail from data imported to our database, back to source. You can see the product of that at\n[Sqwyre.com](https://sqwyre.com).\n\nIn 2021, **whyqd** received financial support from the [Mayor\'s Resilience Fund](https://challenges.org/mayors-resilience/),\nthe Mayor of London\'s £1 million challenge fund to incentivize innovators to address socially impactful issues facing\nLondon. Sqwyre.com will be contributing to research tools needed during post-COVID economic development as part of the\n\'Activating High Streets\' challenge.\n\nThe \'backronym\' for **whyqd** /wɪkɪd/ is _Whythawk Quantitative Data_, [Whythawk](https://whythawk.com)\nis an open data science and open research technical consultancy.\n\n## Licence\n\n[BSD 3](LICENSE)\n',
    'author': 'Gavin Chait',
    'author_email': 'gchait@whythawk.com',
    'maintainer': None,
    'maintainer_email': None,
    'url': 'https://whyqd.com',
    'packages': packages,
    'package_data': package_data,
    'install_requires': install_requires,
    'python_requires': '>=3.7.1,<4.0',
}


setup(**setup_kwargs)
